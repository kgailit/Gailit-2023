{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6a762c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from estnltk import Text\n",
    "from estnltk.taggers import VabamorfTagger\n",
    "from estnltk.taggers import SpellCheckRetagger\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import string \n",
    "import nltk\n",
    "from collections import Counter\n",
    "import re\n",
    "from estnltk.layer_operations import split_by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "321d76d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 'etnc19_web_2019_100000'\n",
    "dest = 'etnc19_web_2019_sõnad'\n",
    "lexic = 'Loendid/töötlemiseks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae268bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "oletamisega_morph_tagger = VabamorfTagger(guess=True, propername=True, disambiguate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63096ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loeb emotikonid sisse\n",
    "emotikonid = []\n",
    "\n",
    "for failinimi in [\"wikipedia_emoticons_list.txt\", \"Unicode_emoticons_list.txt\", \"looks.wtf.txt\", \"unicode_emojis.txt\"]:\n",
    "    with open(\"Loendid/emotikonid/\"+failinimi, \"r\", encoding=\"UTF-8\") as fr:\n",
    "        for line in fr.readlines():\n",
    "            # Väiketähestab\n",
    "            emotikonid.append(line.strip().lower())\n",
    "# Eemaldab korduvad emotikonid\n",
    "emotikonid = list(set(emotikonid))\n",
    "\n",
    "# Mõned emotikonid võivad olla ka kokkukleepumise tõttu olla väärpositiivsed (\":pole\")\n",
    "emotikonid_probleemsed = []\n",
    "with open(\"Loendid/emotikonid/wikipedia_emoticons_sp.txt\", \"r\", encoding=\"UTF-8\") as fr:\n",
    "    for line in fr.readlines():\n",
    "        # Väiketähestab\n",
    "        emotikonid_probleemsed.append(line.strip().lower())\n",
    "# Eemaldab korduvad emotikonid \n",
    "emotikonid_probleemsed = list(set(emotikonid_probleemsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dfa4bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = list()\n",
    "\n",
    "for file in os.listdir(lexic):\n",
    "    with open(os.path.join(lexic, file), \"r\", encoding=\"UTF-8\") as f:\n",
    "        pair = (file.split(\".\")[0], [word.strip() for word in f.readlines()])\n",
    "        pairs.append(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b79cbcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir(source):\n",
    "    with open(os.path.join(source, file), \"r\", encoding=\"UTF-8\") as f:\n",
    "        # Loeb failist vaid sisu, ignoreerib alguses olevat metainfot\n",
    "        pure = \"\".join(f.readlines()[1:])\n",
    "        # Regexiga leiab kõik potentsiaalsed emojid tekstist (kahe kooloni vaheline whitespaceita tekst)\n",
    "        emojid = re.findall(\":\\S+?:\", pure)\n",
    "        # Eemaldab leitud emojide hulgast väärvasted\n",
    "        wrong = [\":http:\", \":https:\"]\n",
    "        for value in wrong:\n",
    "            while value in emojid:\n",
    "                emojid.remove(value)\n",
    "        # Eemaldab emojid tekstist, et nende sisu ei peetaks sõnadeks \n",
    "        for emoji in emojid:\n",
    "            pure = pure.replace(emoji, \"\")\n",
    "\n",
    "        # Otsib tekstist emotikone ja paneb need nimekirja\n",
    "        emotikonid_leitud = []\n",
    "        for emotikon in emotikonid:\n",
    "            emotikonid_leitud.extend(re.findall(re.escape(emotikon), pure, re.IGNORECASE))\n",
    "            # Eemaldab tekstist leitud emotikonid, et need sõnestamisel lahku löömisel need keskmiseid ei mõjutaks\n",
    "            pure = re.sub(re.escape(emotikon), '', pure, re.IGNORECASE)\n",
    "        # Vaatab tekstist eraldi emotikone, mis võivad olla ka kokkukleepumise tulemusel väärpositiivsed vasted\n",
    "        # Lisaks eemaldad leitud emotikonid\n",
    "        sobivad = []\n",
    "        for emotikon in emotikonid_probleemsed:\n",
    "            # Kui \"silmad\" on viimane emotikoni osa, kontrollib, et emotikoni ees ei oleks tegu tähemärgiga ehk et poleks seoses sõnaga\n",
    "            if emotikon[-1] == \":\":\n",
    "                sobivad.extend(re.findall(re.escape(emotikon), \"\\n\".join(re.findall(\"\\W\"+re.escape(emotikon), pure, re.IGNORECASE)), re.IGNORECASE))\n",
    "                pure = re.sub(\"(\\W)\"+re.escape(emotikon), '\\1', pure, re.IGNORECASE)\n",
    "            # Vastasel juhul kontrollib seda emotikoni lõpust\n",
    "            else:\n",
    "                sobivad.extend(re.findall(re.escape(emotikon), \"\\n\".join(re.findall(re.escape(emotikon)+\"\\W\", pure, re.IGNORECASE)), re.IGNORECASE))\n",
    "                pure = re.sub(re.escape(emotikon)+\"(\\W)\", '\\1', pure, re.IGNORECASE)\n",
    "                \n",
    "        oletamisega = Text(pure)\n",
    "        oletamisega.tag_layer(['words', 'sentences', 'compound_tokens'])\n",
    "        oletamisega_morph_tagger.tag( oletamisega )\n",
    "        \n",
    "        # Loeb kokku lemmade arvud ja käänduvate lemmade arvud\n",
    "        kõikide_lemmade_arv = 0\n",
    "        \n",
    "        for lemma, postag in zip(oletamisega.morph_analysis.lemma, oletamisega.morph_analysis.partofspeech):\n",
    "            if postag != \"Z\":\n",
    "                kõikide_lemmade_arv += 1\n",
    "        \n",
    "        dictionary = {}\n",
    "        \n",
    "        for typeOf, _ in pairs:\n",
    "            dictionary[typeOf] = 0\n",
    "            \n",
    "        for lemma, word in zip(oletamisega.morph_analysis.lemma, oletamisega.morph_analysis.normalized_text):\n",
    "            for typeOf, comparisons in pairs:\n",
    "                for comp in comparisons:\n",
    "                    if comp.strip().lower() == lemma[0].strip().lower() or comp.strip().lower() == word[0].strip().lower():\n",
    "                        dictionary[typeOf] += 1\n",
    "                        \n",
    "        for typeOf, _ in pairs:\n",
    "            dictionary[typeOf] = dictionary[typeOf] / kõikide_lemmade_arv\n",
    "            \n",
    "        with open(os.path.join(dest, file), \"w\", encoding=\"UTF-8\") as fw:\n",
    "            json.dump(dictionary, fw, sort_keys=True, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    os.rename(os.path.join(source, file), os.path.join(\"etnc19_web_2019_100000_tehtud\", file))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
