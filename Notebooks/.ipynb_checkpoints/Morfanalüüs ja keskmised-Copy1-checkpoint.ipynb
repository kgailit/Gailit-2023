{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "southeast-economics",
   "metadata": {},
   "outputs": [],
   "source": [
    "from estnltk import Text\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import string \n",
    "import nltk\n",
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raised-field",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "inner-waterproof",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sisendiks faili nimi ja praegune kaust ning sihtkohtade kaustad\n",
    "def morphanalysis_and_save(filename, source, guess_destination, no_guess_destination, meta_destination):\n",
    "    \n",
    "    global keskmiste_arvutamiseks\n",
    "    \n",
    "    with open(os.path.join(source, filename), \"r\", encoding=\"UTF-8\") as f:\n",
    "\n",
    "        faili_keskmised = defaultdict(int)\n",
    "        \n",
    "        # Loeb failist vaid sisu, ignoreerib alguses olevat metainfot\n",
    "        pure = \"\".join(f.readlines()[1:])\n",
    "        \n",
    "        # Regexiga leiab kõik potentsiaalsed emojid tekstist (kahe kooloni vaheline whitespaceita tekst)\n",
    "        emojid = re.findall(\":\\S+?:\", pure)\n",
    "        # Eemaldab leitud emojide hulgast väärvasted\n",
    "        wrong = [\":http:\", \":https:\"]\n",
    "        for value in wrong:\n",
    "            while value in emojid:\n",
    "                emojid.remove(value)\n",
    "        # Eemaldab emojid tekstist, et nende sisu ei peetaks sõnadeks \n",
    "        for emoji in emojid:\n",
    "            pure = pure.replace(emoji, \"\")\n",
    "\n",
    "        # Otsib tekstist emotikone ja paneb need nimekirja\n",
    "        emotikonid_leitud = []\n",
    "        for emotikon in emotikonid:\n",
    "            emotikonid_leitud.extend(re.findall(re.escape(emotikon), pure, re.IGNORECASE))\n",
    "            # Eemaldab tekstist leitud emotikonid, et need sõnestamisel lahku löömisel need keskmiseid ei mõjutaks\n",
    "            pure = re.sub(re.escape(emotikon), '', pure, re.IGNORECASE)\n",
    "        # Vaatab tekstist eraldi emotikone, mis võivad olla ka kokkukleepumise tulemusel väärpositiivsed vasted\n",
    "        # Lisaks eemaldad leitud emotikonid\n",
    "        sobivad = []\n",
    "        for emotikon in emotikonid_probleemsed:\n",
    "            # Kui \"silmad\" on viimane emotikoni osa, kontrollib, et emotikoni ees ei oleks tegu tähemärgiga ehk et poleks seoses sõnaga\n",
    "            if emotikon[-1] == \":\":\n",
    "                sobivad.extend(re.findall(re.escape(emotikon), \"\\n\".join(re.findall(\"\\W\"+re.escape(emotikon), pure, re.IGNORECASE)), re.IGNORECASE))\n",
    "                pure = re.sub(\"(\\W)\"+re.escape(emotikon), '\\1', pure, re.IGNORECASE)\n",
    "            # Vastasel juhul kontrollib seda emotikoni lõpust\n",
    "            else:\n",
    "                sobivad.extend(re.findall(re.escape(emotikon), \"\\n\".join(re.findall(re.escape(emotikon)+\"\\W\", pure, re.IGNORECASE)), re.IGNORECASE))\n",
    "                pure = re.sub(re.escape(emotikon)+\"(\\W)\", '\\1', pure, re.IGNORECASE)\n",
    "        \n",
    "        # Jätab meelde emotikonide arvud ja loendid vigade kontrollimiseks\n",
    "        faili_keskmised['emotikonide_arv']=len(emojid) + len(emotikonid_leitud) + len(sobivad)\n",
    "        faili_keskmised['emojid_loend']=emojid\n",
    "        faili_keskmised['emotikonid_loend']=emotikonid_leitud\n",
    "        faili_keskmised['emotikonid_erilised_loend']=sobivad\n",
    "        \n",
    "        # Teeb morfoloogilise analüüsi nii tundmatude analüüsi oletamisega ja oletamiseta\n",
    "        oletamisega = Text(pure)\n",
    "        oletamiseta = Text(pure, disambiguate=False, guess=False, propername=False)\n",
    "\n",
    "        oletamisega.tag_analysis()\n",
    "        oletamiseta.tag_analysis()\n",
    "        \n",
    "        # Loob salvestamiseks failinimed\n",
    "        ga_name = \".\".join(filename.split(\".\")[:-1]) + \"_morf_oletamisega.json\"\n",
    "        ta_name = \".\".join(filename.split(\".\")[:-1]) + \"_morf_oletamiseta.json\"\n",
    "        meta_name = \".\".join(filename.split(\".\")[:-1]) + \"_meta.json\"\n",
    "        # Salvest EstNLTK objektid failidesse\n",
    "        with open(os.path.join(guess_destination, ga_name), 'w', encoding=\"UTF-8\") as fp:\n",
    "            json.dump(oletamisega, fp, sort_keys=True, indent=4)\n",
    "        with open(os.path.join(no_guess_destination, ta_name), 'w', encoding=\"UTF-8\") as fp:\n",
    "            json.dump(oletamiseta, fp, sort_keys=True, indent=4)\n",
    "        \n",
    "        # Loeb kokku lemmade arvud ja käänduvate lemmade arvud\n",
    "        kõikide_lemmade_arv = 0\n",
    "        ainult_käänduvate_lemmade_arv = 0\n",
    "            \n",
    "        for lemma, postag in zip(oletamisega.lemmas, oletamisega.postags):\n",
    "            kõikide_lemmade_arv += 1\n",
    "            keskmiste_arvutamiseks['kõikide_lemmade_arv'] += 1\n",
    "            # Kui tegu on käänduva lemmaga\n",
    "            if postag in [\"A\", \"C\", \"G\", \"H\", \"K\", \"N\", \"O\", \"S\", \"U\", \"Y\"]:\n",
    "                ainult_käänduvate_lemmade_arv += 1\n",
    "                keskmiste_arvutamiseks['ainult_käänduvate_lemmade_arv'] += 1\n",
    "\n",
    "        # Loeb kokku lemmad lahutades liitsõnad osasõnadeks\n",
    "        lemmas_subwords = []\n",
    "        for tokens in oletamisega.root_tokens:\n",
    "            # Kui tegu on sõnega ja mitte loendiga\n",
    "            if isinstance(tokens[0], str):\n",
    "                for token in tokens:\n",
    "                    # Kui kõik tähemärgid ei ole punktuatsioonimärgid\n",
    "                    if not all(char in string.punctuation for char in token):\n",
    "                        lemmas_subwords.append(token)\n",
    "            # Kui tegu on loendiga, võtab esimese tõlgenduse\n",
    "            else:\n",
    "                for token in tokens[0]:\n",
    "                    # Kui kõik tähemärgid ei ole punktuatsioonimärgid\n",
    "                    if not all(char in string.punctuation for char in token):\n",
    "                        lemmas_subwords.append(token)\n",
    "                        \n",
    "        # Jätab meelde osasõnade pikkused ja arvu\n",
    "        keskmiste_arvutamiseks['lemmade_osasõnade_pikkused'] += sum(map(len, lemmas_subwords))\n",
    "        keskmiste_arvutamiseks['lemmade_osasõnade_arv'] += len(lemmas_subwords)\n",
    "        \n",
    "        # Võtab sõnade algvormid, ignoreerib kirjavahemärke\n",
    "        lemmad = [lemma.split(\"|\")[0] for lemma in oletamisega.lemmas if not all(char in string.punctuation for char in lemma)]\n",
    "        \n",
    "        # Arvutab TTR-i, keskmise lemma osasõna pikkuse ja käänduvate lemmade osaarvu\n",
    "        faili_keskmised['TTR'] = len(Counter(lemmad))/len(lemmad)\n",
    "        faili_keskmised['keskmine_lemma_pikkus'] = sum(map(len, lemmas_subwords))/len(lemmas_subwords)\n",
    "        faili_keskmised['käänduvate_lemmade_osaarv'] = ainult_käänduvate_lemmade_arv/kõikide_lemmade_arv\n",
    "        \n",
    "        # Liidab TTR-id üle failide kokku, et leida tekstide keskmine TTR\n",
    "        keskmiste_arvutamiseks['TTR_keskmine'] += faili_keskmised['TTR']\n",
    "        \n",
    "        # Salvestab faili keskmised ja emotikonide info faili\n",
    "        with open(os.path.join(meta_destination, meta_name), 'w', encoding=\"UTF-8\") as fp:\n",
    "            json.dump(faili_keskmised, fp, sort_keys=True, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "283cd73d",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'Downloads/Bakatöö/Gailit-2021'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-79ad18d95102>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Downloads/Bakatöö/Gailit-2021\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'Downloads/Bakatöö/Gailit-2021'"
     ]
    }
   ],
   "source": [
    "os.chdir(\"Downloads/Bakatöö/Gailit-2021\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b725968d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'os' has no attribute 'dir'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-d86cfd44b672>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdir\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'os' has no attribute 'dir'"
     ]
    }
   ],
   "source": [
    "os.dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "reflected-birmingham",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Loendid/emotikonid/wikipedia_emoticons_list.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-1e461306bb08>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfailinimi\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"wikipedia_emoticons_list.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Unicode_emoticons_list.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"looks.wtf.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"unicode_emojis.txt\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Loendid/emotikonid/\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfailinimi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"UTF-8\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m             \u001b[1;31m# Väiketähestab\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Loendid/emotikonid/wikipedia_emoticons_list.txt'"
     ]
    }
   ],
   "source": [
    "# Loeb emotikonid sisse\n",
    "emotikonid = []\n",
    "\n",
    "for failinimi in [\"wikipedia_emoticons_list.txt\", \"Unicode_emoticons_list.txt\", \"looks.wtf.txt\", \"unicode_emojis.txt\"]:\n",
    "    with open(\"Loendid/emotikonid/\"+failinimi, \"r\", encoding=\"UTF-8\") as fr:\n",
    "        for line in fr.readlines():\n",
    "            # Väiketähestab\n",
    "            emotikonid.append(line.strip().lower())\n",
    "# Eemaldab korduvad emotikonid\n",
    "emotikonid = list(set(emotikonid))\n",
    "\n",
    "# Mõned emotikonid võivad olla ka kokkukleepumise tõttu olla väärpositiivsed (\":pole\")\n",
    "emotikonid_probleemsed = []\n",
    "with open(\"Loendid/emotikonid/wikipedia_emoticons_sp.txt\", \"r\", encoding=\"UTF-8\") as fr:\n",
    "    for line in fr.readlines():\n",
    "        # Väiketähestab\n",
    "        emotikonid_probleemsed.append(line.strip().lower())\n",
    "# Eemaldab korduvad emotikonid \n",
    "emotikonid_probleemsed = list(set(emotikonid_probleemsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "academic-robinson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaustade nimed\n",
    "source = \"etnc19_web_2019_100000/\"\n",
    "target1 = \"etnc19_web_2019_morf_oletamisega/\"\n",
    "target2 = \"etnc19_web_2019_morf_oletamiseta/\"\n",
    "target3 = \"etnc19_web_2019_meta/\"\n",
    "\n",
    "# Loob kaustad\n",
    "os.makedirs(os.path.dirname(target1), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(target2), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(target3), exist_ok=True)\n",
    "\n",
    "# Loob sõnastiku keskmiste arvutamiseks\n",
    "keskmiste_arvutamiseks = defaultdict(int)\n",
    "\n",
    "# Avab järjest kõik failid algkaustas\n",
    "for file in [f for f in os.listdir(source)]:\n",
    "    morphanalysis_and_save(file, source, target1, target2, target3)\n",
    "    \n",
    "# Loob keskmiste sõnastiku\n",
    "keskmised = defaultdict(float)\n",
    "\n",
    "# Arvutab keskmised\n",
    "keskmised['TTR'] = keskmiste_arvutamiseks['TTR_keskmine']/100000\n",
    "keskmised['keskmine_lemma_pikkus'] = keskmiste_arvutamiseks['lemmade_osasõnade_pikkused']/keskmiste_arvutamiseks['lemmade_osasõnade_arv']\n",
    "keskmised['käänduvate_lemmade_osaarv'] = keskmiste_arvutamiseks['ainult_käänduvate_lemmade_arv']/keskmiste_arvutamiseks['kõikide_lemmade_arv']\n",
    "\n",
    "# Salvestab keskmised faili\n",
    "with open(\"keskmised.json\", \"w\", encoding = \"utf-8\") as fw: \n",
    "    json.dump(keskmised, fw, sort_keys=True, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporate-benchmark",
   "metadata": {},
   "source": [
    "Vana info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "modern-belfast",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keskmine sõnapikkus (liitsõnad osadena): 4.27369826435247\n",
      "Type-Token ratio                       : 0.4489510489510489\n",
      "Käändsõna osakaal                      : 0.2987951807228916\n"
     ]
    }
   ],
   "source": [
    "#Andmed\n",
    "print('Keskmine sõnapikkus (liitsõnad osadena): {}'.format(avg_lemma_len))\n",
    "print('Type-Token ratio                       : {}'.format(TTR))\n",
    "print('Käändsõna osakaal                      : {}'.format(kaanduv_suhe_koik))\n",
    "#print('Tajuverbide osakaal verbidest          : {}'.format(tajuverbid_suhe_all_verbs)) # Vaja alles üksikteksti analüüsiks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
