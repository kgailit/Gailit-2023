{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cellular-karaoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "from estnltk import Text\n",
    "from estnltk.taggers import VabamorfTagger\n",
    "from estnltk.taggers import SpellCheckRetagger\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import string \n",
    "import nltk\n",
    "from collections import Counter\n",
    "import re\n",
    "from estnltk.layer_operations import split_by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "reduced-rabbit",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tajuverbide keskmise arvutamine\n",
    "def tajuverbide_keskmine(oletamisega):\n",
    "    with open(\"..\\\\Loendid\\\\tajuverbid\\wordnet_tajuverbid.txt\", \"r\", encoding = \"utf8\") as fr:\n",
    "        lines = fr.readlines()\n",
    "        tajuverbid = [verb.strip() for verb in lines]\n",
    "        \n",
    "    all_verbs = 0\n",
    "    only_tajuverbs = 0\n",
    "\n",
    "    for lemma, postag in zip(oletamisega.morph_analysis.lemma, oletamisega.morph_analysis.partofspeech):\n",
    "        if postag == \"V\":\n",
    "            all_verbs += 1\n",
    "            if lemma in tajuverbid:\n",
    "                only_tajuverbs += 1\n",
    "    # Kui tekstis ei leidu verbe, tagastab -1\n",
    "    if all_verbs == 0:\n",
    "        return -1\n",
    "    return only_tajuverbs / float(all_verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "impossible-newspaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Korduvate tähtede leidmine\n",
    "def leia_korduvad_tähed(oletamisega):\n",
    "    rx = re.compile(r'[^\\d\\s.:,;\\(\\)\\[\\]][.:,;\\(\\)\\[\\]][^\\d\\s.:,;\\(\\)\\[\\]]', re.IGNORECASE)\n",
    "    rxx = rx.findall(oletamisega.text)\n",
    "    return len(rxx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "substantial-penetration",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Korduvate (mitte kokku kleepunud) sõnade leidmine\n",
    "def korduvate_sõnade_arv(oletamisega):\n",
    "    rx = re.compile(r\"(\\b\\w+\\b)(\\s+\\1)+\", re.IGNORECASE)\n",
    "    rxx = rx.findall(oletamisega.text)\n",
    "\n",
    "    return len(rxx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "behavioral-allowance",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kokkukleepunud kirjavahemärkide leidmine\n",
    "def leia_kokkukleepunud_kirjavahemärgid(oletamisega):\n",
    "    rx = re.compile(r'(\\D)\\1{2,}')\n",
    "    rxx = rx.findall(oletamisega.text)\n",
    "    return len(rxx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adult-kansas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leiab kolm korda korduvad vähemalt kahetähelised jupid\n",
    "def leia_korduvad_jupid(oletamisega):\n",
    "    rx = re.compile(r\"([a-zA-ZüÜõÕäÄöÖšŠžŽ])\\1{3,}|([a-zA-ZüÜõÕäÄöÖšŠžŽ]{2,})\\1{2,}\", re.IGNORECASE)\n",
    "    rxx = rx.findall(oletamisega.text)\n",
    "    return len(rxx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "generous-belgium",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tundmatude sõnade osakaalu leidmine\n",
    "def luhemate_tundmatute_osakaal(oletamiseta):\n",
    "    analuusita = 0\n",
    "    koik = 0\n",
    "\n",
    "    for word in oletamiseta.morph_analysis:\n",
    "        if word.lemma[0] == None:\n",
    "            # Vaatab, et ei oleks ainult punktuatsioon\n",
    "            if not all(char in string.punctuation for char in word.text.strip()):\n",
    "                # Kui esitäht on suur, on ilmselt pärisnimi\n",
    "                if word.text[0] == word.text[0].lower():\n",
    "                    if len(word.text) <= 10:\n",
    "                        analuusita += 1\n",
    "                        continue\n",
    "                # Kontrollib, et sõna ei oleks läbinisti suur\n",
    "                # Kui on, eeldab, et pole ikkagi tegu pärisnimega\n",
    "                elif len(word.text) > 1:\n",
    "                    if word.text[1] != word.text[1].lower():\n",
    "                        if len(word.text) <= 10:\n",
    "                            analuusita += 1\n",
    "                else:\n",
    "                    # Ka ühe tähemärgi pikkused tundmatud sõnad märgib analüüsita\n",
    "                    analuusita += 1\n",
    "    # Kui tekstis ei leidu sõnu, tagastab -1\n",
    "    if len(oletamiseta.morph_analysis) == 0:\n",
    "        return -1\n",
    "    return analuusita / len(oletamiseta.morph_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "headed-gathering",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esimese, teise ja kolmanda isiku osakaalu leidmine verbidest\n",
    "def verbide_isikute_osakaalud(oletamisega):\n",
    "    # Tunnuste lõpud, EstNLTK dokumentatsioonist\n",
    "    # Ei kordu omavahel\n",
    "    esi_tunnused = ['sime', 'me', 'nuksime', 'nuksin', 'gem', 'ksin', 'n', 'sin', 'ksime']\n",
    "    teine_tunnused = ['te', 'o', 'nuksite', 'd', 'site', 'ge', 'ksite']\n",
    "    kolmas_tunnused = ['s', 'gu', 'vad', 'b']\n",
    "\n",
    "    # Loeb kokku isikute kaupa ja kõik isikud kokku\n",
    "    arv_koik = 0\n",
    "    arv_esimene = 0\n",
    "    arv_teine = 0\n",
    "    arv_kolmas = 0\n",
    "\n",
    "    # Vaatab iga sõna analüüsi\n",
    "    for analysis in oletamisega.morph_analysis:\n",
    "        # Kui pole mitmese analüüsiga\n",
    "        if len(analysis.lemma) == 1:\n",
    "            # Kui tegu on verbiga\n",
    "            if analysis.partofspeech[0] == \"V\":\n",
    "                #Jätab meelde sõnalõpu\n",
    "                form = analysis.form[0]\n",
    "                # Vaatab, kas tunnus on loendis, ja suurendab vastavalt skoori\n",
    "                if form in esi_tunnused:\n",
    "                    arv_koik += 1\n",
    "                    arv_esimene += 1\n",
    "                    continue\n",
    "                # Vaatab, kas tunnus on loendis, ja suurendab vastavalt skoori\n",
    "                elif form in teine_tunnused:\n",
    "                    arv_koik += 1\n",
    "                    arv_teine += 1\n",
    "                    continue\n",
    "                # Vaatab, kas tunnus on loendis, ja suurendab vastavalt skoori\n",
    "                elif form in kolmas_tunnused:\n",
    "                    arv_koik += 1\n",
    "                    arv_kolmas += 1\n",
    "                    continue\n",
    "    # Kui tekstis ei leidu isikulisi verbe, tagastab -1\n",
    "    if arv_koik == 0:\n",
    "        return -1, -1, -1\n",
    "    # Tagastab kõik kolm osaarvu\n",
    "    return arv_esimene/arv_koik, arv_teine/arv_koik, arv_kolmas/arv_koik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adjacent-rebound",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esimese, teise ja kolmanda isiku osakaalu leidmine asesõnadest\n",
    "def asesonade_isikute_osakaalud(oletamisega):\n",
    "    # Loeb kokku isikute kaupa ja kõik isikud kokku\n",
    "    arv_koik = 0\n",
    "    arv_esimene = 0\n",
    "    arv_teine = 0\n",
    "    arv_kolmas = 0\n",
    "    \n",
    "    # Vaatab iga sõna analüüsi\n",
    "    for analysis in oletamisega.morph_analysis:\n",
    "        # Kui pole mitmese analüüsiga\n",
    "        if len(analysis.lemma) == 1:\n",
    "            # Kui tegu on asesõnaga\n",
    "            if analysis.partofspeech[0] == \"P\":\n",
    "                #Jätab meelde asesõna\n",
    "                lemma = analysis.lemma[0]\n",
    "                #Vaatab asesõna algvormi ja suurendab vastavat skoori\n",
    "                if lemma == 'mina':\n",
    "                    arv_koik += 1\n",
    "                    arv_esimene += 1\n",
    "                    continue\n",
    "                #Vaatab asesõna algvormi ja suurendab vastavat skoori\n",
    "                #Teietamine automaatselt sama, mis sinatamine (sina mitmuse analüüs)\n",
    "                elif lemma == 'sina':\n",
    "                    arv_koik += 1\n",
    "                    arv_teine += 1\n",
    "                    continue\n",
    "                #Vaatab asesõna algvormi ja suurendab vastavat skoori\n",
    "                elif lemma == 'tema':\n",
    "                    arv_koik += 1\n",
    "                    arv_kolmas += 1\n",
    "                    continue\n",
    "    # Kui tekstis ei leidu asesõnu, tagastab -1\n",
    "    if arv_koik == 0:\n",
    "        return -1, -1, -1\n",
    "    # Tagastab kõik kolm osaarvu\n",
    "    return arv_esimene/arv_koik, arv_teine/arv_koik, arv_kolmas/arv_koik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "unnecessary-serve",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Umbisikuliste verbide osakaalu leidmine tekstist\n",
    "def passiivi_osakaal(oletamisega):\n",
    "    # Tunnuste lõpud, EstNLTK dokumentatsioonist\n",
    "    tunnused = ['takse', 'ti', 'tav', 'tuks', 'tagu', 'tama', 'tud', 'tuvat', 'tavat', 'taks', 'ta']\n",
    "    \n",
    "    # Loeb kokku tunnuste kaupa ja kõik kokku\n",
    "    arv_koik = 0\n",
    "    arv_passiiv = 0\n",
    "    \n",
    "    # Vaatab iga sõna analüüsi\n",
    "    for analysis in oletamisega.morph_analysis:\n",
    "        # Kui pole mitmese analüüsiga\n",
    "        if len(analysis.lemma) == 1:\n",
    "            # Kui tegu on verbiga\n",
    "            if analysis.partofspeech[0] == \"V\":\n",
    "                #Jätab meelde sõnalõpu\n",
    "                form = analysis.form[0]\n",
    "                # Vaatab, kas tunnus on loendis, ja suurendab vastavalt skoori\n",
    "                if form in tunnused:\n",
    "                    arv_koik += 1\n",
    "                    arv_passiiv += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    arv_koik += 1\n",
    "        # Kui on mitmese analüüsiga\n",
    "        else:\n",
    "            leidub_passiivne = False\n",
    "            # Vaatab kõiki analüüse\n",
    "            # Kui vähemalt üks on passiivi analüüsiga\n",
    "            # Märgib kogu leiduva vormi passiivseks\n",
    "            for liik, vorm in list(zip(analysis.partofspeech, analysis.form)):\n",
    "                if liik == \"V\":\n",
    "                    # Vaatab, kas tunnus on loendis, ja märgib, et järelikult on passiiviga\n",
    "                    if vorm in tunnused:\n",
    "                        leidub_passiivne = True\n",
    "                        break\n",
    "            \n",
    "            if leidub_passiivne:\n",
    "                arv_koik += 1\n",
    "                arv_passiiv += 1\n",
    "            else:\n",
    "                arv_koik += 1\n",
    "    # Kui tekstis ei leidu verbe, tagastab -1\n",
    "    if arv_koik == 0:\n",
    "        return -1\n",
    "    # Tagastab osaarvu\n",
    "    return arv_passiiv/arv_koik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "valid-franklin",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nud-partitsiibiga verbide osakaalu leidmine tekstist\n",
    "def nud_osakaal(oletamisega):\n",
    "    # Tunnuste lõpud, EstNLTK dokumentatsioonist\n",
    "    tunnused = ['nud']\n",
    "    \n",
    "    # Loeb kokku tunnuste kaupa ja kõik kokku\n",
    "    arv_koik = 0\n",
    "    arv_tunnus = 0\n",
    "    \n",
    "    # Vaatab iga sõna analüüsi\n",
    "    for analysis in oletamisega.morph_analysis:\n",
    "        # Kui pole mitmese analüüsiga\n",
    "        if len(analysis.lemma) == 1:\n",
    "            # Kui tegu on verbiga\n",
    "            if analysis.partofspeech[0] == \"V\":\n",
    "                #Jätab meelde sõnalõpu\n",
    "                form = analysis.form[0]\n",
    "                # Vaatab, kas tunnus on loendis, ja suurendab vastavalt skoori\n",
    "                if form in tunnused:\n",
    "                    arv_koik += 1\n",
    "                    arv_tunnus += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    arv_koik += 1\n",
    "        # Kui on mitmese analüüsiga\n",
    "        else:\n",
    "            leidub_passiivne = False\n",
    "            # Vaatab kõiki analüüse\n",
    "            # Kui vähemalt üks on passiivi analüüsiga\n",
    "            # Märgib kogu leiduva vormi passiivseks\n",
    "            for liik, vorm in list(zip(analysis.partofspeech, analysis.form)):\n",
    "                if liik == \"V\":\n",
    "                    # Vaatab, kas tunnus on loendis, ja märgib, et järelikult on passiiviga\n",
    "                    if vorm in tunnused:\n",
    "                        leidub_passiivne = True\n",
    "                        break\n",
    "            \n",
    "            if leidub_passiivne:\n",
    "                arv_koik += 1\n",
    "                arv_tunnus += 1\n",
    "            else:\n",
    "                arv_koik += 1\n",
    "    # Kui tekstis ei leidu verbe, tagastab -1\n",
    "    if arv_koik == 0:\n",
    "        return -1\n",
    "    # Tagastab osaarvu\n",
    "    return arv_tunnus/arv_koik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "roman-syria",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vat-partitsiibiga verbide osakaalu leidmine tekstist\n",
    "def vat_osakaal(oletamisega):\n",
    "    # Tunnuste lõpud, EstNLTK dokumentatsioonist\n",
    "    tunnused = ['vat']\n",
    "    \n",
    "    # Loeb kokku tunnuste kaupa ja kõik kokku\n",
    "    arv_koik = 0\n",
    "    arv_tunnus = 0\n",
    "    \n",
    "    # Vaatab iga sõna analüüsi\n",
    "    for analysis in oletamisega.morph_analysis:\n",
    "        # Kui pole mitmese analüüsiga\n",
    "        if len(analysis.lemma) == 1:\n",
    "            # Kui tegu on verbiga\n",
    "            if analysis.partofspeech[0] == \"V\":\n",
    "                #Jätab meelde sõnalõpu\n",
    "                form = analysis.form[0]\n",
    "                # Vaatab, kas tunnus on loendis, ja suurendab vastavalt skoori\n",
    "                if form in tunnused:\n",
    "                    arv_koik += 1\n",
    "                    arv_tunnus += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    arv_koik += 1\n",
    "        # Kui on mitmese analüüsiga\n",
    "        else:\n",
    "            leidub_passiivne = False\n",
    "            # Vaatab kõiki analüüse\n",
    "            # Kui vähemalt üks on passiivi analüüsiga\n",
    "            # Märgib kogu leiduva vormi passiivseks\n",
    "            for liik, vorm in list(zip(analysis.partofspeech, analysis.form)):\n",
    "                if liik == \"V\":\n",
    "                    # Vaatab, kas tunnus on loendis, ja märgib, et järelikult on passiiviga\n",
    "                    if vorm in tunnused:\n",
    "                        leidub_passiivne = True\n",
    "                        break\n",
    "            \n",
    "            if leidub_passiivne:\n",
    "                arv_koik += 1\n",
    "                arv_tunnus += 1\n",
    "            else:\n",
    "                arv_koik += 1\n",
    "    # Kui tekstis ei leidu verbe, tagastab -1\n",
    "    if arv_koik == 0:\n",
    "        return -1\n",
    "    # Tagastab osaarvu\n",
    "    return arv_tunnus/arv_koik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055c493f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aerial-guest",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vale_tähesuurus_osakaal(oletamisega):\n",
    "    vale_väike = 0\n",
    "    ainult_suur = 0\n",
    "    # Kõikide sõnade arv (ignoreerib kirjavahemärke)\n",
    "    kõik_arv = 0\n",
    "\n",
    "    # Vaatab iga lauset tekstis ükshaaval\n",
    "    for sentence in split_by(oletamisega, \"sentences\"):\n",
    "        kõik_arv += 1\n",
    "        # Vaatab lause esimest sõna eraldi\n",
    "        # Kas esimene sõna on täis väiketähed või täis suurtähed\n",
    "        if sentence.words[0].text == sentence.words[0].text.lower():\n",
    "            vale_väike += 1\n",
    "        elif sentence.words[0].text == sentence.words[0].text.upper():\n",
    "            # Vaatab, et ei oleks ühe tähe suurune\n",
    "            if len(sentence.words[0].text) > 1:\n",
    "                ainult_suur += 1\n",
    "        # Vaatab iga ülejäänud sõna lauses\n",
    "        for word in sentence.words[1:]:\n",
    "            # Kui sõna on ühe tähe pikkune või ainult kirjavahemärgid, jätab selle sõna vahele\n",
    "            if len(word.text) == 1 or all(char in string.punctuation for char in word.text):\n",
    "                continue\n",
    "            kõik_arv += 1\n",
    "            # Vaatab iga sõna, kas on vaid suurtähed\n",
    "            if word.text == word.text.upper():\n",
    "                # Vaatab, et ei oleks lühendi analüüsiga\n",
    "                lyhend = False\n",
    "                if 'Y' in word.morph_analysis.partofspeech:\n",
    "                    lyhend = True\n",
    "                    break\n",
    "                if not lyhend:\n",
    "                    ainult_suur += 1\n",
    "    # Kui tekstis ei sõnu, tagastab -1\n",
    "    if kõik_arv == 0:\n",
    "        return -1, -1\n",
    "    return vale_väike/kõik_arv, ainult_suur/kõik_arv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "mineral-environment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kirjavigadega_osaarv(oletamisega):\n",
    "    vigadega = Text(oletamisega.text).tag_layer(['words'])\n",
    "    spelling_tagger = SpellCheckRetagger()\n",
    "    spelling_tagger.retag(vigadega)\n",
    "\n",
    "    kirjavigadega = 0\n",
    "    # Vaatleb EstNLTK-sse sisse ehitatud kirjavigade mooduli abil kirjavigu\n",
    "    for vigane, oletamine in zip(vigadega.words, split_by(oletamisega, \"words\")):\n",
    "        # Kui tegu on kirjaveaga sõnaga\n",
    "        if list(vigane.normalized_form) != [None]:\n",
    "            nimi = False\n",
    "            # Vaatab, et tegu poleks pärisnimega\n",
    "            for analysis in oletamine.morph_analysis:\n",
    "                if 'H' in list(analysis.partofspeech):\n",
    "                    nimi = True\n",
    "            if not nimi:\n",
    "                # Kui pole nimi, määrab kirjaveaks\n",
    "                kirjavigadega += 1\n",
    "    # Tagastab kirjavigadega sõnade protsendi\n",
    "    return kirjavigadega / len(oletamisega.words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "reverse-surrey",
   "metadata": {},
   "outputs": [],
   "source": [
    "def faili_info_salvestamine(filename):\n",
    "    global keskmised\n",
    "    \n",
    "    # Loob failinimed\n",
    "    ga_name = os.path.join(\"etnc19_web_2019_morf_oletamisega/\", \".\".join(filename.split(\".\")[:-1]) + \"_morf_oletamisega.json\")\n",
    "    ta_name = os.path.join(\"etnc19_web_2019_morf_oletamiseta/\", \".\".join(filename.split(\".\")[:-1]) + \"_morf_oletamiseta.json\")\n",
    "    meta_name = os.path.join(\"etnc19_web_2019_meta/\", \".\".join(filename.split(\".\")[:-1]) + \"_meta.json\")\n",
    "    \n",
    "    andmed_name = os.path.join(\"etnc19_web_2019_andmed/\", \".\".join(filename.split(\".\")[:-1]) + \"_meta.json\")\n",
    "    \n",
    "    # Loeb failidest sisse tekstid ja metainfo\n",
    "    oletamisega = None\n",
    "    oletamiseta = None\n",
    "    meta_info = None\n",
    "    with open(ga_name, \"r\", encoding = \"utf-8\") as fr:\n",
    "        oletamisega = Text(json.load(fr))\n",
    "    with open(ta_name, \"r\", encoding = \"utf-8\") as fr:\n",
    "        oletamiseta = Text(json.load(fr))\n",
    "    with open(meta_name, \"r\", encoding = \"utf-8\") as fr:\n",
    "        meta_info = json.load(fr)\n",
    "    \n",
    "    # Väljastatavate andmete sõnastik\n",
    "    andmed = defaultdict(float)\n",
    "    \n",
    "    # Metainfost oluliste andmete meelde jätmine\n",
    "    andmed[\"emotikonide_arv\"] = meta_info[\"emotikonide_arv\"]\n",
    "    andmed[\"TTR\"] = meta_info[\"TTR\"]\n",
    "    andmed[\"käändsõnade_osaarv\"] = meta_info[\"käänduvate_lemmade_osaarv\"]\n",
    "    andmed[\"lemmapikkuse_osaarv\"] = meta_info[\"keskmine_lemma_pikkus\"]\n",
    "    \n",
    "    # Arvutab tajuverbide osaarvu kõikidest verbidest ja jätab meelde\n",
    "    andmed['tajuverbide_osaarv'] = tajuverbide_keskmine(oletamisega)\n",
    "    \n",
    "    # Arvutab lühikeste tundmatu morfanalüüsi saanud sõnade osaarvu\n",
    "    andmed['luhemate_tundmatute_osakaal'] = luhemate_tundmatute_osakaal(oletamiseta)\n",
    "    \n",
    "    # Loendab kokku vähemalt kolm korda korduvad tähed sõna sees\n",
    "    andmed['korduvate_tähtede_arv'] = leia_korduvad_tähed(oletamisega)\n",
    "    \n",
    "    # Loendab kokku sõnadesse kokku kleepunud kirjavahemärkide arvu\n",
    "    andmed['kokkukleepunud_kirjavahemärkide_arv'] = leia_kokkukleepunud_kirjavahemärgid(oletamisega)\n",
    "    \n",
    "    # Leiab verbide isikute protsendid (kui palju on 1., 2. ja 3. isik)\n",
    "    verbide_isikute_protsendid = verbide_isikute_osakaalud(oletamisega)\n",
    "    andmed['verbide_esimese_isiku_osaarv'] = verbide_isikute_protsendid[0]\n",
    "    andmed['verbide_teise_isiku_osaarv'] = verbide_isikute_protsendid[1]\n",
    "    andmed['verbide_kolmanda_isiku_osaarv'] = verbide_isikute_protsendid[2]\n",
    "    \n",
    "    # Leiab asesõnade isikute protsendid (kui palju on 1., 2. ja 3. isik)\n",
    "    asesonade_isikute_protsendid = asesonade_isikute_osakaalud(oletamisega)\n",
    "    andmed['asesõnade_esimese_isiku_osaarv'] = asesonade_isikute_protsendid[0]\n",
    "    andmed['asesõnade_teise_isiku_osaarv'] = asesonade_isikute_protsendid[1]\n",
    "    andmed['asesõnade_kolmanda_isiku_osaarv'] = asesonade_isikute_protsendid[2]\n",
    "    \n",
    "    # Leiab passiivi osakaalu ja jätab meelde\n",
    "    andmed['passiivi_osakaal'] = passiivi_osakaal(oletamisega)\n",
    "    \n",
    "    # Leiab nud-partitsiibi osakaalu ja jätab meelde\n",
    "    andmed['nud-partitsiibiga_verbide_osakaal'] = nud_osakaal(oletamisega)\n",
    "    \n",
    "    # Leiab kaudse kõneviisi osakaalu ja jätab meelde\n",
    "    andmed['kaudse_kõneviisi_osakaal'] = vat_osakaal(oletamisega)\n",
    "    \n",
    "    # Leiab, kui palju tekstist on väikese algustähega või läbinisti suur\n",
    "    valed_suurused = vale_tähesuurus_osakaal(oletamisega)\n",
    "    andmed['puuduva_suure_algustähega'] = valed_suurused[0]\n",
    "    andmed['läbinisti_suur'] = valed_suurused[1]\n",
    "    \n",
    "    # Leiab korduvate sõnade arvu\n",
    "    andmed['korduvate_sõnade_arv'] = korduvate_sõnade_arv(oletamisega)\n",
    "    \n",
    "    # Leiab korduvate kokkukleepunud \"juppide\" arvu\n",
    "    # nt silbid aga ka muud arbitraarsed kordused üle 2 tähe ja 3 korduse\n",
    "    andmed['korduvate_juppide_arv'] = leia_korduvad_jupid(oletamisega)\n",
    "    \n",
    "    # Leiab kirjavigadega sõnade (mitte nimede) osaarvu\n",
    "    andmed['kirjavigadega_osaarv'] = kirjavigadega_osaarv(oletamisega)\n",
    "    \n",
    "    # Kirjutab metainfo tagasi faili edasiseks skoori arvutamiseks\n",
    "    with open(andmed_name, 'w', encoding=\"UTF-8\") as fw:\n",
    "        json.dump(andmed, fw, sort_keys=True, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "compliant-bruce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formaalsus(andmed):\n",
    "    skoor = 0\n",
    "    \n",
    "    tunnus_TTR = andmed[\"TTR\"]\n",
    "    # Kui tunnust tekstis ei leidunud, ei mõjuta see skoori\n",
    "    if tunnus_TTR == -1:\n",
    "        skoor += 0\n",
    "    # Jagasin polaarse tunnuse korpuse alusel 11-ks võrdseks osaks, et hinnata\n",
    "    # <= 9.1%\n",
    "    elif tunnus_TTR <= 0.450412:\n",
    "        skoor -= 5\n",
    "    # <= 18.2%\n",
    "    elif tunnus_TTR <= 0.520466:\n",
    "        skoor -= 4\n",
    "    # <= 27.3%\n",
    "    elif tunnus_TTR <= 0.571429:\n",
    "        skoor -= 3\n",
    "    # <= 36.3%\n",
    "    elif tunnus_TTR <= 0.611948:\n",
    "        skoor -= 2\n",
    "    # <= 45.4%\n",
    "    elif tunnus_TTR <= 0.647191:\n",
    "        skoor -= 1\n",
    "    # <= 54.5%\n",
    "    elif tunnus_TTR <= 0.679739:\n",
    "        skoor += 0\n",
    "    # <= 63.6%\n",
    "    elif tunnus_TTR <= 0.709795:\n",
    "        skoor += 1\n",
    "    # <= 72.7%\n",
    "    elif tunnus_TTR <= 0.740260:\n",
    "        skoor += 2\n",
    "    # <= 81.8%\n",
    "    elif tunnus_TTR <= 0.772973:\n",
    "        skoor += 3\n",
    "    # <= 90.9%\n",
    "    elif tunnus_TTR <= 0.813333:\n",
    "        skoor += 4\n",
    "    # <= 100% (ka suuremad, kui korpuses leiduvad)\n",
    "    else:\n",
    "        skoor += 5\n",
    "    \n",
    "    skoor_isik = 0\n",
    "    \n",
    "    tunnus_a1i = andmed[\"asesõnade_esimese_isiku_osaarv\"]\n",
    "    # Kui tunnust tekstis ei leidunud, ei mõjuta see skoori\n",
    "    if tunnus_a1i == -1:\n",
    "        skoor_isik -= 0\n",
    "    # Jagasin mitte-polaarse tunnuse korpuse alusel 6-ks võrdseks osaks, et hinnata\n",
    "    # <= 16.7%\n",
    "    elif tunnus_a1i <= 0.000000:\n",
    "        skoor_isik -= 0\n",
    "    # <= 33.3%\n",
    "    elif tunnus_a1i <= 0.242424:\n",
    "        skoor_isik -= 1\n",
    "    # <= 50%\n",
    "    elif tunnus_a1i <= 0.500000:\n",
    "        skoor_isik -= 2\n",
    "    # <= 66.7%\n",
    "    elif tunnus_a1i <= 0.666667:\n",
    "        skoor_isik -= 3\n",
    "    # <= 83.3%\n",
    "    elif tunnus_a1i <= 0.894737:\n",
    "        skoor_isik -= 4\n",
    "    # <= 100% (ka suuremad, kui korpuses leiduvad)\n",
    "    else:\n",
    "        skoor_isik -= 5\n",
    "    \n",
    "    tunnus_a3i = andmed[\"asesõnade_kolmanda_isiku_osaarv\"]\n",
    "    # Kui tunnust tekstis ei leidunud, ei mõjuta see skoori\n",
    "    if tunnus_a3i == -1:\n",
    "        skoor_isik += 0\n",
    "    # Jagasin mitte-polaarse tunnuse korpuse alusel 6-ks võrdseks osaks, et hinnata\n",
    "    # <= 33.3%\n",
    "    elif tunnus_a3i == 0.000000:\n",
    "        skoor_isik += 0\n",
    "    # <= 50%\n",
    "    elif tunnus_a3i <= 0.200000:\n",
    "        skoor_isik += 2\n",
    "    # <= 66.7%\n",
    "    elif tunnus_a3i <= 0.416667:\n",
    "        skoor_isik += 3\n",
    "    # <= 83.3%\n",
    "    elif tunnus_a3i <= 0.800000:\n",
    "        skoor_isik += 4\n",
    "    # <= 100% (ka suuremad, kui korpuses leiduvad)\n",
    "    else:\n",
    "        skoor_isik += 5\n",
    "    \n",
    "    tunnus_a2i = andmed[\"asesõnade_teise_isiku_osaarv\"]\n",
    "    # Kui tunnust tekstis ei leidunud, ei mõjuta see skoori\n",
    "    if tunnus_a2i == -1:\n",
    "        skoor_isik -= 0\n",
    "    # Jagasin mitte-polaarse tunnuse korpuse alusel 6-ks võrdseks osaks, et hinnata\n",
    "    # <= 50%\n",
    "    elif tunnus_a2i == 0.000000:\n",
    "        skoor_isik -= 1\n",
    "    # <= 66.7%\n",
    "    elif tunnus_a2i <= 0.200000:\n",
    "        skoor_isik -= 3\n",
    "    # <= 83.3%\n",
    "    elif tunnus_a2i <= 0.500000:\n",
    "        skoor_isik -= 4\n",
    "    # <= 100% (ka suuremad, kui korpuses leiduvad)\n",
    "    else:\n",
    "        skoor_isik -= 5\n",
    "    \n",
    "    tunnus_v1i = andmed[\"verbide_esimese_isiku_osaarv\"]\n",
    "    # Kui tunnust tekstis ei leidunud, ei mõjuta see skoori\n",
    "    if tunnus_v1i == -1:\n",
    "        skoor_isik -= 0\n",
    "    # Jagasin mitte-polaarse tunnuse korpuse alusel 6-ks võrdseks osaks, et hinnata\n",
    "    # <= 33.3%\n",
    "    elif tunnus_v1i == 0.000000:\n",
    "        skoor_isik -= 1\n",
    "    # <= 50%\n",
    "    elif tunnus_v1i <= 0.105263:\n",
    "        skoor_isik -= 2\n",
    "    # <= 66.7%\n",
    "    elif tunnus_v1i <= 0.222222:\n",
    "        skoor_isik -= 3\n",
    "    # <= 83.3%\n",
    "    elif tunnus_v1i <= 0.375000:\n",
    "        skoor_isik -= 4\n",
    "    # <= 100% (ka suuremad, kui korpuses leiduvad)\n",
    "    else:\n",
    "        skoor_isik -= 5\n",
    "    \n",
    "    tunnus_v2i = andmed[\"verbide_teise_isiku_osaarv\"]\n",
    "    # Kui tunnust tekstis ei leidunud, ei mõjuta see skoori\n",
    "    if tunnus_v2i == -1:\n",
    "        skoor_isik -= 0\n",
    "    # Jagasin mitte-polaarse tunnuse korpuse alusel 6-ks võrdseks osaks, et hinnata\n",
    "    # <= 16.7%\n",
    "    elif tunnus_v2i <= 0.000000:\n",
    "        skoor_isik -= 0\n",
    "    # <= 33.3%\n",
    "    elif tunnus_v2i <= 0.058824:\n",
    "        skoor_isik -= 1\n",
    "    # <= 50%\n",
    "    elif tunnus_v2i <= 0.125000:\n",
    "        skoor_isik -= 2\n",
    "    # <= 66.7%\n",
    "    elif tunnus_v2i <= 0.200000:\n",
    "        skoor_isik -= 3\n",
    "    # <= 83.3%\n",
    "    elif tunnus_v2i <= 0.333333:\n",
    "        skoor_isik -= 4\n",
    "    # <= 100% (ka suuremad, kui korpuses leiduvad)\n",
    "    else:\n",
    "        skoor_isik -= 5\n",
    "    \n",
    "    tunnus_v3i = andmed[\"verbide_kolmanda_isiku_osaarv\"]\n",
    "    # Kui tunnust tekstis ei leidunud, ei mõjuta see skoori\n",
    "    if tunnus_v3i == -1:\n",
    "        skoor_isik += 0\n",
    "    # Jagasin mitte-polaarse tunnuse korpuse alusel 6-ks võrdseks osaks, et hinnata\n",
    "    # <= 16.7%\n",
    "    elif tunnus_v3i <= 0.400000:\n",
    "        skoor_isik += 0\n",
    "    # <= 33.3%\n",
    "    elif tunnus_v3i <= 0.536585:\n",
    "        skoor_isik += 1\n",
    "    # <= 50%\n",
    "    elif tunnus_v3i <= 0.666667:\n",
    "        skoor_isik += 2\n",
    "    # <= 66.7%\n",
    "    elif tunnus_v3i <= 0.800000:\n",
    "        skoor_isik += 3\n",
    "    # <= 83.3%\n",
    "    elif tunnus_v3i <= 0.945946:\n",
    "        skoor_isik += 4\n",
    "    # <= 100% (ka suuremad, kui korpuses leiduvad)\n",
    "    else:\n",
    "        skoor_isik += 5\n",
    "    \n",
    "    # Vähendan isikute mõju skoorile, kuna need on omavahel seotud\n",
    "    skoor += (skoor_isik / 6)\n",
    "    \n",
    "    tunnus_emotikonide_arv = andmed[\"emotikonide_arv\"]\n",
    "    # Binaarse tunnuse (on või ei ole) skoor on vastavalt kas 5 või 0\n",
    "    if tunnus_emotikonide_arv > 0.0000:\n",
    "        skoor -= 5\n",
    "    \n",
    "    tunnus_kaudse_kõneviisi_osakaal = andmed[\"kaudse_kõneviisi_osakaal\"]\n",
    "    # Binaarse tunnuse (on või ei ole) skoor on vastavalt kas 5 või 0\n",
    "    if tunnus_kaudse_kõneviisi_osakaal > 0.0000:\n",
    "        skoor += 5\n",
    "    \n",
    "    tunnus_nud = andmed[\"nud-partitsiibiga_verbide_osakaal\"]\n",
    "        # Kui tunnust tekstis ei leidunud, ei mõjuta see skoori\n",
    "    if tunnus_nud == -1:\n",
    "        skoor_isik -= 0\n",
    "    # Jagasin mitte-polaarse tunnuse korpuse alusel 6-ks võrdseks osaks, et hinnata\n",
    "    # <= 16.7%\n",
    "    elif tunnus_nud == 0.000000:\n",
    "        skoor -= 0\n",
    "    # <= 33.3%\n",
    "    elif tunnus_nud <= 0.011429:\n",
    "        skoor -= 1\n",
    "    # <= 50%\n",
    "    elif tunnus_nud <= 0.031250:\n",
    "        skoor -= 2\n",
    "    # <= 66.7%\n",
    "    elif tunnus_nud <= 0.049180:\n",
    "        skoor -= 3\n",
    "    # <= 83.3%\n",
    "    elif tunnus_nud <= 0.073529:\n",
    "        skoor -= 4\n",
    "    # <= 100% (ka suuremad, kui korpuses leiduvad)\n",
    "    else:\n",
    "        skoor -= 5\n",
    "    \n",
    "    tunnus_passiiv = andmed[\"passiivi_osakaal\"]\n",
    "    # Kui tunnust tekstis ei leidunud, ei mõjuta see skoori\n",
    "    if tunnus_passiiv == -1:\n",
    "        skoor_isik += 0\n",
    "    # Jagasin mitte-polaarse tunnuse korpuse alusel 6-ks võrdseks osaks, et hinnata\n",
    "    # <= 16.7%\n",
    "    elif tunnus_passiiv <= 0.019608:\n",
    "        skoor += 0\n",
    "    # <= 33.3%\n",
    "    elif tunnus_passiiv <= 0.043011:\n",
    "        skoor += 1\n",
    "    # <= 50%\n",
    "    elif tunnus_passiiv <= 0.065246:\n",
    "        skoor += 2\n",
    "    # <= 66.7%\n",
    "    elif tunnus_passiiv <= 0.095808:\n",
    "        skoor += 3\n",
    "    # <= 83.3%\n",
    "    elif tunnus_passiiv <= 0.148138:\n",
    "        skoor += 4\n",
    "    # <= 100% (ka suuremad, kui korpuses leiduvad)\n",
    "    else:\n",
    "        skoor += 5\n",
    "    \n",
    "    tunnus_käändsõnad = andmed[\"käändsõnade_osaarv\"]\n",
    "    # Kui tunnust tekstis ei leidunud, ei mõjuta see skoori\n",
    "    if tunnus_käändsõnad == -1:\n",
    "        skoor += 0\n",
    "    # Jagasin polaarse tunnuse korpuse alusel 11-ks võrdseks osaks, et hinnata\n",
    "    # <= 9.1%\n",
    "    elif tunnus_käändsõnad <= 0.333333:\n",
    "        skoor -= 5\n",
    "    # <= 18.2%\n",
    "    elif tunnus_käändsõnad <= 0.378481:\n",
    "        skoor -= 4\n",
    "    # <= 27.3%\n",
    "    elif tunnus_käändsõnad <= 0.415044:\n",
    "        skoor -= 3\n",
    "    # <= 36.4%\n",
    "    elif tunnus_käändsõnad <= 0.447115:\n",
    "        skoor -= 2\n",
    "    # <= 45.5%\n",
    "    elif tunnus_käändsõnad <= 0.475958:\n",
    "        skoor -= 1\n",
    "    # <= 54.5%\n",
    "    elif tunnus_käändsõnad <= 0.502941:\n",
    "        skoor += 0\n",
    "    # <= 63.6%\n",
    "    elif tunnus_käändsõnad <= 0.527778:\n",
    "        skoor += 1\n",
    "    # <= 72.7%\n",
    "    elif tunnus_käändsõnad <= 0.552941:\n",
    "        skoor += 2\n",
    "    # <= 81.8%\n",
    "    elif tunnus_käändsõnad <= 0.581395:\n",
    "        skoor += 3\n",
    "    # <= 90.9%\n",
    "    elif tunnus_käändsõnad <= 0.619570:\n",
    "        skoor += 4\n",
    "    # <= 100% (ka suuremad, kui korpuses leiduvad)\n",
    "    else:\n",
    "        skoor += 5\n",
    "    \n",
    "    tunnus_lemmapikkus = andmed[\"lemmapikkuse_osaarv\"]\n",
    "    # Kui tunnust tekstis ei leidunud, ei mõjuta see skoori\n",
    "    if tunnus_lemmapikkus == -1:\n",
    "        skoor += 0\n",
    "    # Jagasin polaarse tunnuse korpuse alusel 11-ks võrdseks osaks, et hinnata\n",
    "    # <= 9.1%\n",
    "    elif tunnus_lemmapikkus <= 4.401786:\n",
    "        skoor -= 5\n",
    "    # <= 18.2%\n",
    "    elif tunnus_lemmapikkus <= 4.543703:\n",
    "        skoor -= 4\n",
    "    # <= 27.3%\n",
    "    elif tunnus_lemmapikkus <= 4.657800:\n",
    "        skoor -= 3\n",
    "    # <= 36.4%\n",
    "    elif tunnus_lemmapikkus <= 4.760417:\n",
    "        skoor -= 2\n",
    "    # <= 45.5%\n",
    "    elif tunnus_lemmapikkus <= 4.854926:\n",
    "        skoor -= 1\n",
    "    # <= 54.5%\n",
    "    elif tunnus_lemmapikkus <= 4.949524:\n",
    "        skoor += 0\n",
    "    # <= 63.6%\n",
    "    elif tunnus_lemmapikkus <= 5.045977:\n",
    "        skoor += 1\n",
    "    # <= 72.7%\n",
    "    elif tunnus_lemmapikkus <= 5.153620:\n",
    "        skoor += 2\n",
    "    # <= 81.8%\n",
    "    elif tunnus_lemmapikkus <= 5.282090:\n",
    "        skoor += 3\n",
    "    # <= 90.9%\n",
    "    elif tunnus_lemmapikkus <= 5.468645:\n",
    "        skoor += 4\n",
    "    # <= 100% (ka suuremad, kui korpuses leiduvad)\n",
    "    else:\n",
    "        skoor += 5\n",
    "    \n",
    "    #Paneb skoori vahemikku -1 ja 1\n",
    "    if skoor < 0:\n",
    "        return skoor / (5+5+5+5+5 + (20/6))\n",
    "    else:\n",
    "        return skoor / (5+5+5+5+5 + (10/6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "consistent-product",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spontaansus(andmed):\n",
    "    skoor = 0\n",
    "    \n",
    "    tunnus_TTR = andmed[\"TTR\"]\n",
    "    # Kui tunnust tekstis ei leidunud, ei mõjuta see skoori\n",
    "    if tunnus_TTR == -1:\n",
    "        skoor += 0\n",
    "    # Jagasin polaarse tunnuse korpuse alusel 11-ks võrdseks osaks, et hinnata\n",
    "    # <= 9.1%\n",
    "    elif tunnus_TTR <= 0.450412:\n",
    "        skoor += 5\n",
    "    # <= 18.2%\n",
    "    elif tunnus_TTR <= 0.520466:\n",
    "        skoor += 4\n",
    "    # <= 27.3%\n",
    "    elif tunnus_TTR <= 0.571429:\n",
    "        skoor += 3\n",
    "    # <= 36.3%\n",
    "    elif tunnus_TTR <= 0.611948:\n",
    "        skoor += 2\n",
    "    # <= 45.4%\n",
    "    elif tunnus_TTR <= 0.647191:\n",
    "        skoor += 1\n",
    "    # <= 54.5%\n",
    "    elif tunnus_TTR <= 0.679739:\n",
    "        skoor += 0\n",
    "    # <= 63.6%\n",
    "    elif tunnus_TTR <= 0.709795:\n",
    "        skoor -= 1\n",
    "    # <= 72.7%\n",
    "    elif tunnus_TTR <= 0.740260:\n",
    "        skoor -= 2\n",
    "    # <= 81.8%\n",
    "    elif tunnus_TTR <= 0.772973:\n",
    "        skoor -= 3\n",
    "    # <= 90.9%\n",
    "    elif tunnus_TTR <= 0.813333:\n",
    "        skoor -= 4\n",
    "    # <= 100% (ka suuremad, kui korpuses leiduvad)\n",
    "    else:\n",
    "        skoor -= 5\n",
    "    \n",
    "    tunnus_emotikonide_arv = andmed[\"emotikonide_arv\"]\n",
    "    # Binaarse tunnuse (on või ei ole) skoor on vastavalt kas 5 või 0\n",
    "    if tunnus_emotikonide_arv > 0.0000:\n",
    "        skoor += 5\n",
    "    \n",
    "    skoor_kirjavead = 0\n",
    "    \n",
    "    tunnus_kirjavigadega = andmed[\"kirjavigadega_osaarv\"]\n",
    "    # Kui tunnust tekstis ei leidunud, ei mõjuta see skoori\n",
    "    if tunnus_kirjavigadega == -1:\n",
    "        skoor += 0\n",
    "    # Jagasin mitte-polaarse tunnuse korpuse alusel 6-ks võrdseks osaks, et hinnata\n",
    "    # Ei käsitle kirjavigasid binaarsetena, kuna neid võib olla vale-positiivseid (nt pärisnimesid)\n",
    "    # <= 16.7%\n",
    "    elif tunnus_kirjavigadega == 0.000000:\n",
    "        skoor_kirjavead += 0\n",
    "    # <= 33.3%\n",
    "    elif tunnus_kirjavigadega <= 0.007508:\n",
    "        skoor_kirjavead += 1\n",
    "    # <= 50%\n",
    "    elif tunnus_kirjavigadega <= 0.012698:\n",
    "        skoor_kirjavead += 2\n",
    "    # <= 66.7%\n",
    "    elif tunnus_kirjavigadega <= 0.020690:\n",
    "        skoor_kirjavead += 3\n",
    "    # <= 83.3%\n",
    "    elif tunnus_kirjavigadega <= 0.035088:\n",
    "        skoor_kirjavead += 4\n",
    "    # <= 100% (ka suuremad, kui korpuses leiduvad)\n",
    "    else:\n",
    "        skoor_kirjavead += 5\n",
    "        \n",
    "    tunnus_luhemate_tundmatute_osakaal = andmed[\"luhemate_tundmatute_osakaal\"]\n",
    "    # Kui tunnust tekstis ei leidunud, ei mõjuta see skoori\n",
    "    if tunnus_kirjavigadega == -1:\n",
    "        skoor += 0\n",
    "    # Jagasin mitte-polaarse tunnuse korpuse alusel 6-ks võrdseks osaks, et hinnata\n",
    "    # Ei käsitle kirjavigasid binaarsetena, kuna neid võib olla vale-positiivseid (nt pärisnimesid)\n",
    "    # <= 16.7%\n",
    "    elif tunnus_luhemate_tundmatute_osakaal == 0.000000:\n",
    "        skoor_kirjavead += 0\n",
    "    # <= 33.3%\n",
    "    elif tunnus_luhemate_tundmatute_osakaal <= 0.002433:\n",
    "        skoor_kirjavead += 1\n",
    "    # <= 50%\n",
    "    elif tunnus_luhemate_tundmatute_osakaal <= 0.006711:\n",
    "        skoor_kirjavead += 2\n",
    "    # <= 66.7%\n",
    "    elif tunnus_luhemate_tundmatute_osakaal <= 0.011628:\n",
    "        skoor_kirjavead += 3\n",
    "    # <= 83.3%\n",
    "    elif tunnus_luhemate_tundmatute_osakaal <= 0.02170:\n",
    "        skoor_kirjavead += 4\n",
    "    # <= 100% (ka suuremad, kui korpuses leiduvad)\n",
    "    else:\n",
    "        skoor_kirjavead += 5\n",
    "    \n",
    "    skoor += (skoor_kirjavead / 2)\n",
    "    \n",
    "    tunnus_lemmapikkus = andmed[\"lemmapikkuse_osaarv\"]\n",
    "    # Kui tunnust tekstis ei leidunud, ei mõjuta see skoori\n",
    "    if tunnus_lemmapikkus == -1:\n",
    "        skoor += 0\n",
    "    # Jagasin polaarse tunnuse korpuse alusel 11-ks võrdseks osaks, et hinnata\n",
    "    # <= 9.1%\n",
    "    elif tunnus_lemmapikkus <= 4.401786:\n",
    "        skoor += 5\n",
    "    # <= 18.2%\n",
    "    elif tunnus_lemmapikkus <= 4.543703:\n",
    "        skoor += 4\n",
    "    # <= 27.3%\n",
    "    elif tunnus_lemmapikkus <= 4.657800:\n",
    "        skoor += 3\n",
    "    # <= 36.4%\n",
    "    elif tunnus_lemmapikkus <= 4.760417:\n",
    "        skoor += 2\n",
    "    # <= 45.5%\n",
    "    elif tunnus_lemmapikkus <= 4.854926:\n",
    "        skoor += 1\n",
    "    # <= 54.5%\n",
    "    elif tunnus_lemmapikkus <= 4.949524:\n",
    "        skoor += 0\n",
    "    # <= 63.6%\n",
    "    elif tunnus_lemmapikkus <= 5.045977:\n",
    "        skoor -= 1\n",
    "    # <= 72.7%\n",
    "    elif tunnus_lemmapikkus <= 5.153620:\n",
    "        skoor -= 2\n",
    "    # <= 81.8%\n",
    "    elif tunnus_lemmapikkus <= 5.282090:\n",
    "        skoor -= 3\n",
    "    # <= 90.9%\n",
    "    elif tunnus_lemmapikkus <= 5.468645:\n",
    "        skoor -= 4\n",
    "    # <= 100% (ka suuremad, kui korpuses leiduvad)\n",
    "    else:\n",
    "        skoor -= 5\n",
    "    \n",
    "    tunnus_tajuverbide_osaarv = andmed[\"tajuverbide_osaarv\"]\n",
    "    # Kui tunnust tekstis ei leidunud, ei mõjuta see skoori\n",
    "    if tunnus_tajuverbide_osaarv == -1:\n",
    "        skoor += 0\n",
    "    # Jagasin mitte-polaarse tunnuse korpuse alusel 6-ks võrdseks osaks, et hinnata\n",
    "    # <= 16.7%\n",
    "    elif tunnus_tajuverbide_osaarv <= 0.044118:\n",
    "        skoor += 0\n",
    "    # <= 33.3%\n",
    "    elif tunnus_tajuverbide_osaarv <= 0.085714:\n",
    "        skoor += 1\n",
    "    # <= 50%\n",
    "    elif tunnus_tajuverbide_osaarv <= 0.117647:\n",
    "        skoor += 2\n",
    "    # <= 66.7%\n",
    "    elif tunnus_tajuverbide_osaarv <= 0.150000:\n",
    "        skoor += 3\n",
    "    # <= 83.3%\n",
    "    elif tunnus_tajuverbide_osaarv <= 0.192652:\n",
    "        skoor += 4\n",
    "    # <= 100% (ka suuremad, kui korpuses leiduvad)\n",
    "    else:\n",
    "        skoor += 5\n",
    "    \n",
    "    tunnus_kokkukleepunud_kirjavahemärkide_arv = andmed[\"kokkukleepunud_kirjavahemärkide_arv\"]\n",
    "    # Binaarse tunnuse (on või ei ole) skoor on vastavalt kas 5 või 0\n",
    "    if tunnus_luhemate_tundmatute_osakaal > 0.0000:\n",
    "        skoor += 5\n",
    "    \n",
    "    tunnus_korduvate_juppide_arv = andmed[\"korduvate_juppide_arv\"]\n",
    "    # Binaarse tunnuse (on või ei ole) skoor on vastavalt kas 5 või 0\n",
    "    if tunnus_korduvate_juppide_arv > 0.0000:\n",
    "        skoor += 5\n",
    "    \n",
    "    tunnus_korduvate_sõnade_arv = andmed[\"korduvate_sõnade_arv\"]\n",
    "    # Binaarse tunnuse (on või ei ole) skoor on vastavalt kas 5 või 0\n",
    "    if tunnus_korduvate_sõnade_arv > 0.0000:\n",
    "        skoor += 5\n",
    "    \n",
    "    tunnus_läbinisti_suur = andmed[\"läbinisti_suur\"]\n",
    "    # Kui tunnust tekstis ei leidunud, ei mõjuta see skoori\n",
    "    if tunnus_läbinisti_suur == -1:\n",
    "        skoor += 0\n",
    "    # Jagasin mitte-polaarse tunnuse korpuse alusel 6-ks võrdseks osaks, et hinnata\n",
    "    # Ei käsitle kirjavigasid binaarsetena, kuna neid võib olla vale-positiivseid (nt pärisnimesid)\n",
    "    # <= 16.7%\n",
    "    elif tunnus_läbinisti_suur <= 0.003876:\n",
    "        skoor += 0\n",
    "    # <= 33.3%\n",
    "    elif tunnus_läbinisti_suur <= 0.010945:\n",
    "        skoor += 1\n",
    "    # <= 50%\n",
    "    elif tunnus_läbinisti_suur <= 0.018692:\n",
    "        skoor += 2\n",
    "    # <= 66.7%\n",
    "    elif tunnus_läbinisti_suur <= 0.030612:\n",
    "        skoor += 3\n",
    "    # <= 83.3%\n",
    "    elif tunnus_läbinisti_suur <= 0.053410:\n",
    "        skoor += 4\n",
    "    # <= 100% (ka suuremad, kui korpuses leiduvad)\n",
    "    else:\n",
    "        skoor += 5\n",
    "    \n",
    "    tunnus_puuduva_suure_algustähega = andmed[\"puuduva_suure_algustähega\"]\n",
    "    # Binaarse tunnuse (on või ei ole) skoor on vastavalt kas 5 või 0\n",
    "    if tunnus_puuduva_suure_algustähega > 0.0000:\n",
    "        skoor += 5\n",
    "    \n",
    "    #Paneb skoori vahemikku -1 ja 1, kuid 0 jääb 0-ks\n",
    "    if skoor < 0:\n",
    "        return skoor / (5 + 5)\n",
    "    else:\n",
    "        return skoor / (5 + 5 + (10/2) + 5 + 5 + 5 + 5 + 5 + 5 + 5 + 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "provincial-programming",
   "metadata": {},
   "source": [
    "Jooksutamisele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "colonial-chemistry",
   "metadata": {},
   "outputs": [],
   "source": [
    "def morphanalysis_and_save(filename, source, target1, target2):\n",
    "    # Avab faili\n",
    "    with open(os.path.join(source, filename), \"r\", encoding=\"UTF-8\") as f:\n",
    "        # Loeb failist vaid sisu, ignoreerib alguses olevat metainfot\n",
    "        pure = \"\".join(f.readlines())\n",
    "        # Regexiga leiab kõik potentsiaalsed emojid tekstist (kahe kooloni vaheline whitespaceita tekst)\n",
    "        emojid = re.findall(\":\\S+?:\", pure)\n",
    "        # Eemaldab leitud emojide hulgast väärvasted\n",
    "        wrong = [\":http:\", \":https:\"]\n",
    "        for value in wrong:\n",
    "            while value in emojid:\n",
    "                emojid.remove(value)\n",
    "        # Eemaldab emojid tekstist, et nende sisu ei peetaks sõnadeks \n",
    "        for emoji in emojid:\n",
    "            pure = pure.replace(emoji, \"\")\n",
    "\n",
    "        # Otsib tekstist emotikone ja paneb need nimekirja\n",
    "        emotikonid_leitud = []\n",
    "        for emotikon in emotikonid:\n",
    "            emotikonid_leitud.extend(re.findall(re.escape(emotikon), pure, re.IGNORECASE))\n",
    "            # Eemaldab tekstist leitud emotikonid, et need sõnestamisel lahku löömisel need keskmiseid ei mõjutaks\n",
    "            pure = re.sub(re.escape(emotikon), '', pure, re.IGNORECASE)\n",
    "        # Vaatab tekstist eraldi emotikone, mis võivad olla ka kokkukleepumise tulemusel väärpositiivsed vasted\n",
    "        # Lisaks eemaldad leitud emotikonid\n",
    "        sobivad = []\n",
    "        for emotikon in emotikonid_probleemsed:\n",
    "            # Kui \"silmad\" on viimane emotikoni osa, kontrollib, et emotikoni ees ei oleks tegu tähemärgiga ehk et poleks seoses sõnaga\n",
    "            if emotikon[-1] == \":\":\n",
    "                sobivad.extend(re.findall(re.escape(emotikon), \"\\n\".join(re.findall(\"\\W\"+re.escape(emotikon), pure, re.IGNORECASE)), re.IGNORECASE))\n",
    "                pure = re.sub(\"(\\W)\"+re.escape(emotikon), '\\1', pure, re.IGNORECASE)\n",
    "            # Vastasel juhul kontrollib seda emotikoni lõpust\n",
    "            else:\n",
    "                sobivad.extend(re.findall(re.escape(emotikon), \"\\n\".join(re.findall(re.escape(emotikon)+\"\\W\", pure, re.IGNORECASE)), re.IGNORECASE))\n",
    "                pure = re.sub(re.escape(emotikon)+\"(\\W)\", '\\1', pure, re.IGNORECASE)\n",
    "\n",
    "        # Väljastatavate andmete sõnastik\n",
    "        andmed = defaultdict(float)\n",
    "\n",
    "        # Jätab meelde emotikonide arvud ja loendid vigade kontrollimiseks\n",
    "        andmed['emotikonide_arv'] = len(emojid) + len(emotikonid_leitud) + len(sobivad)\n",
    "\n",
    "        # Teeb morfoloogilise analüüsi nii tundmatude analüüsi oletamisega ja oletamiseta\n",
    "        oletamisega = Text(pure)\n",
    "        oletamiseta = Text(pure)\n",
    "\n",
    "        oletamisega.tag_layer(['words', 'sentences', 'compound_tokens'])\n",
    "        oletamiseta.tag_layer(['words', 'sentences', 'compound_tokens'])\n",
    "\n",
    "        oletamisega_morph_tagger.tag( oletamisega )\n",
    "        oletamiseta_morph_tagger.tag( oletamiseta )\n",
    "\n",
    "        # Loeb kokku lemmade arvud ja käänduvate lemmade arvud\n",
    "        kõikide_lemmade_arv = 0\n",
    "        ainult_käänduvate_lemmade_arv = 0\n",
    "\n",
    "        for lemma, postag in zip(oletamisega.morph_analysis.lemma, oletamisega.morph_analysis.partofspeech):\n",
    "            kõikide_lemmade_arv += 1\n",
    "            # Kui tegu on käänduva lemmaga\n",
    "            if postag[0] in [\"A\", \"C\", \"G\", \"H\", \"K\", \"N\", \"O\", \"S\", \"U\", \"Y\"]:\n",
    "                ainult_käänduvate_lemmade_arv += 1\n",
    "\n",
    "        # Loeb kokku lemmad lahutades liitsõnad osasõnadeks\n",
    "        lemmas_subwords = []\n",
    "        for tokens in oletamisega.morph_analysis.root_tokens:\n",
    "            # Kui tegu on sõnega ja mitte loendiga\n",
    "            if isinstance(tokens[0], str):\n",
    "                for token in tokens:\n",
    "                    # Kui kõik tähemärgid ei ole punktuatsioonimärgid\n",
    "                    if not all(char in string.punctuation for char in token):\n",
    "                        lemmas_subwords.append(token)\n",
    "            # Kui tegu on loendiga, võtab esimese tõlgenduse\n",
    "            else:\n",
    "                for token in tokens[0]:\n",
    "                    # Kui kõik tähemärgid ei ole punktuatsioonimärgid\n",
    "                    if not all(char in string.punctuation for char in token):\n",
    "                        lemmas_subwords.append(token)\n",
    "                        \n",
    "        # Võtab sõnade algvormid, ignoreerib kirjavahemärke\n",
    "        lemmad = [lemma[0] for lemma in oletamisega.morph_analysis.lemma if not all(char in string.punctuation for char in lemma)]\n",
    "            \n",
    "        # Arvutab TTR-i, keskmise lemma osasõna pikkuse ja käänduvate lemmade osaarvu\n",
    "        andmed['TTR'] = len(Counter(lemmad))/len(lemmad)\n",
    "        andmed['lemmapikkuse_osaarv'] = sum(map(len, lemmas_subwords))/len(lemmas_subwords)\n",
    "        andmed['käändsõnade_osaarv'] = ainult_käänduvate_lemmade_arv/kõikide_lemmade_arv\n",
    "\n",
    "        # Arvutab tajuverbide osaarvu kõikidest verbidest ja jätab meelde\n",
    "        andmed['tajuverbide_osaarv'] = tajuverbide_keskmine(oletamisega)\n",
    "\n",
    "        # Arvutab lühikeste tundmatu morfanalüüsi saanud sõnade osaarvu\n",
    "        andmed['luhemate_tundmatute_osakaal'] = luhemate_tundmatute_osakaal(oletamiseta)\n",
    "\n",
    "        # Loendab kokku vähemalt kolm korda korduvad tähed sõna sees\n",
    "        andmed['korduvate_tähtede_arv'] = leia_korduvad_tähed(oletamisega)\n",
    "\n",
    "        # Loendab kokku sõnadesse kokku kleepunud kirjavahemärkide arvu\n",
    "        andmed['kokkukleepunud_kirjavahemärkide_arv'] = leia_kokkukleepunud_kirjavahemärgid(oletamisega)\n",
    "\n",
    "        # Leiab verbide isikute protsendid (kui palju on 1., 2. ja 3. isik)\n",
    "        verbide_isikute_protsendid = verbide_isikute_osakaalud(oletamisega)\n",
    "        andmed['verbide_esimese_isiku_osaarv'] = verbide_isikute_protsendid[0]\n",
    "        andmed['verbide_teise_isiku_osaarv'] = verbide_isikute_protsendid[1]\n",
    "        andmed['verbide_kolmanda_isiku_osaarv'] = verbide_isikute_protsendid[2]\n",
    "\n",
    "        # Leiab asesõnade isikute protsendid (kui palju on 1., 2. ja 3. isik)\n",
    "        asesonade_isikute_protsendid = asesonade_isikute_osakaalud(oletamisega)\n",
    "        andmed['asesõnade_esimese_isiku_osaarv'] = asesonade_isikute_protsendid[0]\n",
    "        andmed['asesõnade_teise_isiku_osaarv'] = asesonade_isikute_protsendid[1]\n",
    "        andmed['asesõnade_kolmanda_isiku_osaarv'] = asesonade_isikute_protsendid[2]\n",
    "\n",
    "        # Leiab passiivi osakaalu ja jätab meelde\n",
    "        andmed['passiivi_osakaal'] = passiivi_osakaal(oletamisega)\n",
    "\n",
    "        # Leiab nud-partitsiibi osakaalu ja jätab meelde\n",
    "        andmed['nud-partitsiibiga_verbide_osakaal'] = nud_osakaal(oletamisega)\n",
    "\n",
    "        # Leiab kaudse kõneviisi osakaalu ja jätab meelde\n",
    "        andmed['kaudse_kõneviisi_osakaal'] = vat_osakaal(oletamisega)\n",
    "\n",
    "        # Leiab, kui palju tekstist on väikese algustähega või läbinisti suur\n",
    "        valed_suurused = vale_tähesuurus_osakaal(oletamisega)\n",
    "        andmed['puuduva_suure_algustähega'] = valed_suurused[0]\n",
    "        andmed['läbinisti_suur'] = valed_suurused[1]\n",
    "\n",
    "        # Leiab korduvate sõnade arvu\n",
    "        andmed['korduvate_sõnade_arv'] = korduvate_sõnade_arv(oletamisega)\n",
    "\n",
    "        # Leiab korduvate kokkukleepunud \"juppide\" arvu\n",
    "        # nt silbid aga ka muud arbitraarsed kordused üle 2 tähe ja 3 korduse\n",
    "        andmed['korduvate_juppide_arv'] = leia_korduvad_jupid(oletamisega)\n",
    "\n",
    "        # Leiab kirjavigadega sõnade (mitte nimede) osaarvu\n",
    "        andmed['kirjavigadega_osaarv'] = kirjavigadega_osaarv(oletamisega)\n",
    "\n",
    "        # Teisendab tunnused skoorideks\n",
    "        #skoorid = dict()\n",
    "\n",
    "        #skoorid[\"formaalsus\"] = formaalsus(andmed)\n",
    "        #skoorid[\"spontaansus\"] = spontaansus(andmed)\n",
    "\n",
    "        #Skoor faili või skoor ekraanile\n",
    "        #target_filename = os.path.join(\"etnc19_web_2019_skoorid/\", file)\n",
    "        \n",
    "        #with open(os.path.join(target1, filename), \"w\", encoding=\"UTF-8\") as fw:\n",
    "            #json.dump(skoorid, fw, sort_keys=True, indent=4, ensure_ascii=False)\n",
    "            \n",
    "        with open(os.path.join(target2, filename), \"w\", encoding=\"UTF-8\") as fw:\n",
    "            json.dump(andmed, fw, sort_keys=True, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "assisted-gibson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loeb emotikonid sisse\n",
    "emotikonid = []\n",
    "\n",
    "for failinimi in [\"wikipedia_emoticons_list.txt\", \"Unicode_emoticons_list.txt\", \"looks.wtf.txt\", \"unicode_emojis.txt\"]:\n",
    "    with open(\"../Loendid/emotikonid/\"+failinimi, \"r\", encoding=\"UTF-8\") as fr:\n",
    "        for line in fr.readlines():\n",
    "            # Väiketähestab\n",
    "            emotikonid.append(line.strip().lower())\n",
    "# Eemaldab korduvad emotikonid\n",
    "emotikonid = list(set(emotikonid))\n",
    "\n",
    "# Mõned emotikonid võivad olla ka kokkukleepumise tõttu olla väärpositiivsed (\":pole\")\n",
    "emotikonid_probleemsed = []\n",
    "with open(\"../Loendid/emotikonid/wikipedia_emoticons_sp.txt\", \"r\", encoding=\"UTF-8\") as fr:\n",
    "    for line in fr.readlines():\n",
    "        # Väiketähestab\n",
    "        emotikonid_probleemsed.append(line.strip().lower())\n",
    "# Eemaldab korduvad emotikonid \n",
    "emotikonid_probleemsed = list(set(emotikonid_probleemsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09842e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "oletamiseta_morph_tagger = VabamorfTagger(guess=False, propername=False, disambiguate=False)\n",
    "oletamisega_morph_tagger = VabamorfTagger(guess=True, propername=True, disambiguate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaptive-joining",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaustade nimed\n",
    "source = \"../etnc19_web_2019_100000\"\n",
    "target1 = \"../etnc19_web_2019_skoor\"\n",
    "target2 = \"../etnc19_web_2019_meta\"\n",
    "\n",
    "# Avab järjest kõik failid algkaustas\n",
    "for file in [f for f in os.listdir(source)]:\n",
    "    morphanalysis_and_save(file, source, target1, target2)\n",
    "    os.rename(os.path.join(source, file), os.path.join(\"../etnc19_web_2019_100000_tehtud\", file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5251c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avab järjest kõik failid algkaustas\n",
    "for file in [f for f in os.listdir(target2)]:\n",
    "    with open(os.path.join(source, filename), \"r\", encoding=\"UTF-8\") as f:\n",
    "        andmed = json.load(f)\n",
    "        \n",
    "        # Teisendab tunnused skoorideks\n",
    "        skoorid = dict()\n",
    "\n",
    "        skoorid[\"formaalsus\"] = formaalsus(andmed)\n",
    "        skoorid[\"spontaansus\"] = spontaansus(andmed)\n",
    "\n",
    "        #Skoor faili või skoor ekraanile\n",
    "        target_filename = os.path.join(\"etnc19_web_2019_skoorid/\", file)\n",
    "        \n",
    "        with open(os.path.join(target1, filename), \"w\", encoding=\"UTF-8\") as fw:\n",
    "            json.dump(skoorid, fw, sort_keys=True, indent=4, ensure_ascii=False)\n",
    "    os.rename(os.path.join(target2, file), os.path.join(\"../etnc19_web_2019_meta_tehtud\", file))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
