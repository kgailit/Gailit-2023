{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "driving-amino",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "from estnltk import Span, Layer, Text\n",
    "from estnltk.converters import text_to_json, json_to_text\n",
    "from estnltk.taggers import VabamorfTagger\n",
    "\n",
    "import json\n",
    "import math\n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "neither-bracket",
   "metadata": {},
   "outputs": [],
   "source": [
    "def faili_info_salvestamine(filename):\n",
    "    global keskmised\n",
    "    \n",
    "    andmed_name = os.path.join(target, \".\".join(filename.split(\".\")[:-1]) + \"_andmed.json\")\n",
    "    \n",
    "    # Väljastatavate andmete sõnastik\n",
    "    andmed = defaultdict(float)\n",
    "    \n",
    "    with open(os.path.join(source, filename), \"r\", encoding=\"UTF-8\") as f:\n",
    "        # Loeb failist sisu\n",
    "        pure = \"\".join(f.readlines())\n",
    "        \n",
    "    # Regexiga leiab kõik potentsiaalsed emojid tekstist (kahe kooloni vaheline whitespaceita tekst)\n",
    "    emojid = re.findall(\":\\S+?:\", pure)\n",
    "    # Eemaldab leitud emojide hulgast väärvasted\n",
    "    wrong = [\":http:\", \":https:\"]\n",
    "    for value in wrong:\n",
    "        while value in emojid:\n",
    "            emojid.remove(value)\n",
    "    # Eemaldab emojid tekstist, et nende sisu ei peetaks sõnadeks \n",
    "    for emoji in emojid:\n",
    "        pure = pure.replace(emoji, \"\")\n",
    "\n",
    "    # Otsib tekstist emotikone ja paneb need nimekirja\n",
    "    emotikonid_leitud = []\n",
    "    for emotikon in emotikonid:\n",
    "        emotikonid_leitud.extend(re.findall(re.escape(emotikon), pure, re.IGNORECASE))\n",
    "        # Eemaldab tekstist leitud emotikonid, et need sõnestamisel lahku löömisel need keskmiseid ei mõjutaks\n",
    "        pure = re.sub(re.escape(emotikon), '', pure, re.IGNORECASE)\n",
    "    # Vaatab tekstist eraldi emotikone, mis võivad olla ka kokkukleepumise tulemusel väärpositiivsed vasted\n",
    "    # Lisaks eemaldad leitud emotikonid\n",
    "    sobivad = []\n",
    "    for emotikon in emotikonid_probleemsed:\n",
    "        # Kui \"silmad\" on viimane emotikoni osa, kontrollib, et emotikoni ees ei oleks tegu tähemärgiga ehk et poleks seoses sõnaga\n",
    "        if emotikon[-1] == \":\":\n",
    "            sobivad.extend(re.findall(re.escape(emotikon), \"\\n\".join(re.findall(\"\\W\"+re.escape(emotikon), pure, re.IGNORECASE)), re.IGNORECASE))\n",
    "            pure = re.sub(\"(\\W)\"+re.escape(emotikon), '\\1', pure, re.IGNORECASE)\n",
    "        # Vastasel juhul kontrollib seda emotikoni lõpust\n",
    "        else:\n",
    "            sobivad.extend(re.findall(re.escape(emotikon), \"\\n\".join(re.findall(re.escape(emotikon)+\"\\W\", pure, re.IGNORECASE)), re.IGNORECASE))\n",
    "            pure = re.sub(re.escape(emotikon)+\"(\\W)\", '\\1', pure, re.IGNORECASE)\n",
    "        \n",
    "    # Jätab meelde emotikonide arvud ja loendid vigade kontrollimiseks\n",
    "    andmed['emotikonide_arv']=len(emojid) + len(emotikonid_leitud) + len(sobivad)\n",
    "        \n",
    "    # Teeb morfoloogilise analüüsi nii tundmatude analüüsi oletamisega ja oletamiseta\n",
    "    oletamisega = Text(pure)\n",
    "        \n",
    "    oletamisega.tag_layer(['words', 'sentences', 'compound_tokens'])\n",
    "        \n",
    "    oletamisega_morph_tagger.tag( oletamisega )\n",
    "    \n",
    "    # Loeb kokku lemmade arvud ja käänduvate lemmade arvud\n",
    "    kõikide_lemmade_arv = 0\n",
    "    ainult_käänduvate_lemmade_arv = 0\n",
    "    \n",
    "    for lemma, postag in zip(oletamisega.lemma, oletamisega.partofspeech):\n",
    "        kõikide_lemmade_arv += 1\n",
    "        # Kui tegu on käänduva lemmaga\n",
    "        # Vaatab ainult esimest sõnaliiki (et välistada käändelsi verbivorme omadussõnade hulgast)\n",
    "        if postag[0] in [\"A\", \"C\", \"G\", \"H\", \"K\", \"N\", \"O\", \"P\", \"S\", \"U\", \"Y\"]:\n",
    "            ainult_käänduvate_lemmade_arv += 1\n",
    "\n",
    "    # Loeb kokku lemmad lahutades liitsõnad osasõnadeks\n",
    "    lemmas_subwords = []\n",
    "    for tokens in oletamisega.root_tokens:\n",
    "        lemmad = None\n",
    "        # Võtab lemmade loendist esimese tõlgenduse:\n",
    "        lemmad = tokens[0]\n",
    "        # Vaatab iga lemmat tekstis\n",
    "        for lemma in lemmad:\n",
    "            # Kui kõik tähemärgid ei ole punktuatsioonimärgid\n",
    "            if not all(char in string.punctuation for char in lemma):\n",
    "                lemmas_subwords.append(lemma)\n",
    "        \n",
    "    # Võtab sõnade algvormid, ignoreerib kirjavahemärke\n",
    "    lemmad = [lemma[0] for lemma in oletamisega.lemma  if not all(char in string.punctuation for char in lemma)]\n",
    "        \n",
    "    # Arvutab TTR-i, keskmise lemma osasõna pikkuse ja käänduvate lemmade osaarvu\n",
    "    andmed['TTR'] = len(Counter(lemmad))/len(lemmad)\n",
    "    andmed['lemmapikkuse_osaarv'] = sum(map(len, lemmas_subwords))/len(lemmas_subwords)\n",
    "    andmed['käändsõnade_osaarv'] = ainult_käänduvate_lemmade_arv/kõikide_lemmade_arv\n",
    "    \n",
    "    # Arvutab tajuverbide osaarvu kõikidest verbidest ja jätab meelde\n",
    "    andmed['tajuverbide_osaarv'] = tajuverbide_keskmine(oletamisega)\n",
    "    \n",
    "    # Loendab kokku vähemalt kolm korda korduvad tähed sõna sees\n",
    "    andmed['korduvate_tähtede_arv'] = leia_korduvad_tähed(oletamisega)\n",
    "    \n",
    "    # Loendab kokku sõnadesse kokku kleepunud kirjavahemärkide arvu\n",
    "    andmed['kokkukleepunud_kirjavahemärkide_arv'] = leia_kokkukleepunud_kirjavahemärgid(oletamisega)\n",
    "    \n",
    "    # Leiab verbide isikute protsendid (kui palju on 1., 2. ja 3. isik)\n",
    "    verbide_isikute_protsendid = verbide_isikute_osakaalud(oletamisega)\n",
    "    andmed['verbide_esimese_isiku_osaarv'] = verbide_isikute_protsendid[0]\n",
    "    andmed['verbide_teise_isiku_osaarv'] = verbide_isikute_protsendid[1]\n",
    "    andmed['verbide_kolmanda_isiku_osaarv'] = verbide_isikute_protsendid[2]\n",
    "    \n",
    "    # Leiab asesõnade isikute protsendid (kui palju on 1., 2. ja 3. isik)\n",
    "    asesonade_isikute_protsendid = asesonade_isikute_osakaalud(oletamisega)\n",
    "    andmed['asesõnade_esimese_isiku_osaarv'] = asesonade_isikute_protsendid[0]\n",
    "    andmed['asesõnade_teise_isiku_osaarv'] = asesonade_isikute_protsendid[1]\n",
    "    andmed['asesõnade_kolmanda_isiku_osaarv'] = asesonade_isikute_protsendid[2]\n",
    "    \n",
    "    # Leiab passiivi osakaalu ja jätab meelde\n",
    "    andmed['passiivi_osakaal'] = passiivi_osakaal(oletamisega)\n",
    "    \n",
    "    # Leiab nud-partitsiibi osakaalu ja jätab meelde\n",
    "    andmed['nud-partitsiibiga_verbide_osakaal'] = nud_osakaal(oletamisega)\n",
    "    \n",
    "    # Leiab kaudse kõneviisi osakaalu ja jätab meelde\n",
    "    andmed['kaudse_kõneviisi_osakaal'] = vat_osakaal(oletamisega)\n",
    "    \n",
    "    # Leiab, kui palju tekstist on väikese algustähega või läbinisti suur\n",
    "    valed_suurused = vale_tähesuurus_osakaal(oletamisega)\n",
    "    andmed['puuduva_suure_algustähega'] = valed_suurused[0]\n",
    "    andmed['läbinisti_suur'] = valed_suurused[1]\n",
    "    \n",
    "    # Leiab korduvate sõnade arvu\n",
    "    andmed['korduvate_sõnade_arv'] = korduvate_sõnade_arv(oletamisega)\n",
    "    \n",
    "    # Leiab korduvate kokkukleepunud \"juppide\" arvu\n",
    "    # nt silbid aga ka muud arbitraarsed kordused üle 2 tähe ja 3 korduse\n",
    "    andmed['korduvate_juppide_arv'] = leia_korduvad_jupid(oletamisega)\n",
    "    \n",
    "    andmed['leksikonides_esinevade_osaarv'] = sõnaloendi_osaarv(oletamisega)\n",
    "    \n",
    "    # Kirjutab metainfo tagasi faili edasiseks skoori arvutamiseks\n",
    "    with open(andmed_name, 'w', encoding=\"UTF-8\") as fw:\n",
    "        json.dump(andmed, fw, sort_keys=True, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aerial-frost",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tajuverbide keskmise arvutamine\n",
    "def tajuverbide_keskmine(oletamisega):        \n",
    "    all_verbs = 0\n",
    "    only_tajuverbs = 0\n",
    "\n",
    "    for lemma, postag in zip(oletamisega.lemma, oletamisega.partofspeech):\n",
    "        if \"V\" in postag:\n",
    "            all_verbs += 1\n",
    "            for lemmake in lemma:\n",
    "                if lemmake in tajuverbid:\n",
    "                    only_tajuverbs += 1\n",
    "    # Kui tekstis ei leidu verbe, tagastab -1\n",
    "    if all_verbs == 0:\n",
    "        return -1\n",
    "    return only_tajuverbs / float(all_verbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "protected-maldives",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Korduvate tähtede leidmine\n",
    "def leia_korduvad_tähed(oletamisega):\n",
    "    rx = re.compile(r'[^\\d\\s.:,;\\(\\)\\[\\]][.:,;\\(\\)\\[\\]][^\\d\\s.:,;\\(\\)\\[\\]]', re.IGNORECASE)\n",
    "    rxx = rx.findall(oletamisega.text)\n",
    "    return len(rxx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "demanding-paintball",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Korduvate (mitte kokku kleepunud) sõnade leidmine\n",
    "def korduvate_sõnade_arv(oletamisega):\n",
    "    rx = re.compile(r\"(\\b\\w+\\b)(\\s+\\1)+\", re.IGNORECASE)\n",
    "    rxx = rx.findall(oletamisega.text)\n",
    "\n",
    "    return len(rxx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "meaning-closer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kokkukleepunud kirjavahemärkide leidmine\n",
    "def leia_kokkukleepunud_kirjavahemärgid(oletamisega):\n",
    "    rx = re.compile(r'\\D\\s?[,.!?][^\\d\\s]')\n",
    "    rxx = rx.findall(oletamisega.text)\n",
    "    return len(rxx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sunrise-brand",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leiab kolm korda korduvad vähemalt kahetähelised jupid\n",
    "def leia_korduvad_jupid(oletamisega):\n",
    "    rx = re.compile(r\"([a-zA-ZüÜõÕäÄöÖšŠžŽ])\\1{3,}|([a-zA-ZüÜõÕäÄöÖšŠžŽ]{2,})\\1{2,}\", re.IGNORECASE)\n",
    "    rxx = rx.findall(oletamisega.text)\n",
    "    return len(rxx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "collect-sodium",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esimese, teise ja kolmanda isiku osakaalu leidmine verbidest\n",
    "def verbide_isikute_osakaalud(oletamisega):\n",
    "    # Tunnuste lõpud, EstNLTK dokumentatsioonist\n",
    "    # Ei kordu omavahel\n",
    "    esi_tunnused = ['sime', 'me', 'nuksime', 'nuksin', 'gem', 'ksin', 'n', 'sin', 'ksime']\n",
    "    teine_tunnused = ['te', 'o', 'nuksite', 'd', 'site', 'ge', 'ksite']\n",
    "    kolmas_tunnused = ['s', 'gu', 'vad', 'b']\n",
    "    \n",
    "    # Loeb kokku isikute kaupa ja kõik isikud kokku\n",
    "    arv_koik = 0\n",
    "    arv_esimene = 0\n",
    "    arv_teine = 0\n",
    "    arv_kolmas = 0\n",
    "    \n",
    "    # Vaatab iga sõna analüüsi\n",
    "    for analysis in oletamisega.morph_analysis:\n",
    "        # Kui pole mitmese analüüsiga\n",
    "        if len(analysis.partofspeech) == 1:\n",
    "            pos = analysis.partofspeech[0]\n",
    "            # Kui tegu on verbiga\n",
    "            if pos == \"V\":\n",
    "                #Jätab meelde sõnalõpu\n",
    "                form = analysis.form[0]\n",
    "                # Vaatab, kas tunnus on loendis, ja suurendab vastavalt skoori\n",
    "                if form in esi_tunnused:\n",
    "                    arv_koik += 1\n",
    "                    arv_esimene += 1\n",
    "                    continue\n",
    "                # Vaatab, kas tunnus on loendis, ja suurendab vastavalt skoori\n",
    "                elif form in teine_tunnused:\n",
    "                    arv_koik += 1\n",
    "                    arv_teine += 1\n",
    "                    continue\n",
    "                # Vaatab, kas tunnus on loendis, ja suurendab vastavalt skoori\n",
    "                elif form in kolmas_tunnused:\n",
    "                    arv_koik += 1\n",
    "                    arv_kolmas += 1\n",
    "                    continue\n",
    "    # Kui tekstis ei leidu isikulisi verbe, tagastab -1\n",
    "    if arv_koik == 0:\n",
    "        return -1, -1, -1\n",
    "    # Tagastab kõik kolm osaarvu\n",
    "    return arv_esimene/arv_koik, arv_teine/arv_koik, arv_kolmas/arv_koik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "sapphire-psychiatry",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Esimese, teise ja kolmanda isiku osakaalu leidmine asesõnadest\n",
    "def asesonade_isikute_osakaalud(oletamisega):\n",
    "    # Loeb kokku isikute kaupa ja kõik isikud kokku\n",
    "    arv_koik = 0\n",
    "    arv_esimene = 0\n",
    "    arv_teine = 0\n",
    "    arv_kolmas = 0\n",
    "    \n",
    "    # Vaatab iga sõna analüüsi\n",
    "    for analysis in oletamisega.morph_analysis:\n",
    "        # Kui pole mitmese analüüsiga\n",
    "        if len(analysis.partofspeech) == 1:\n",
    "            pos = analysis.partofspeech[0]\n",
    "            # Kui tegu on asesõnaga\n",
    "            if pos == \"P\":\n",
    "                #Jätab meelde asesõna\n",
    "                lemma = analysis.lemma[0]\n",
    "                #Vaatab asesõna algvormi ja suurendab vastavat skoori\n",
    "                if lemma == 'mina':\n",
    "                    arv_koik += 1\n",
    "                    arv_esimene += 1\n",
    "                    continue\n",
    "                #Vaatab asesõna algvormi ja suurendab vastavat skoori\n",
    "                #Teietamine automaatselt sama, mis sinatamine (sina mitmuse analüüs)\n",
    "                elif lemma == 'sina':\n",
    "                    arv_koik += 1\n",
    "                    arv_teine += 1\n",
    "                    continue\n",
    "                #Vaatab asesõna algvormi ja suurendab vastavat skoori\n",
    "                elif lemma == 'tema':\n",
    "                    arv_koik += 1\n",
    "                    arv_kolmas += 1\n",
    "                    continue\n",
    "    # Kui tekstis ei leidu asesõnu, tagastab -1\n",
    "    if arv_koik == 0:\n",
    "        return -1, -1, -1\n",
    "    # Tagastab kõik kolm osaarvu\n",
    "    return arv_esimene/arv_koik, arv_teine/arv_koik, arv_kolmas/arv_koik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "subtle-sauce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Umbisikuliste verbide osakaalu leidmine tekstist\n",
    "def passiivi_osakaal(oletamisega):\n",
    "    # Tunnuste lõpud, EstNLTK dokumentatsioonist\n",
    "    tunnused = ['takse', 'ti', 'tav', 'tuks', 'tagu', 'tama', 'tud', 'tuvat', 'tavat', 'taks', 'ta']\n",
    "    \n",
    "    # Loeb kokku tunnuste kaupa ja kõik kokku\n",
    "    arv_koik = 0\n",
    "    arv_passiiv = 0\n",
    "    \n",
    "    # Vaatab iga sõna analüüsi\n",
    "    for analysis in oletamisega.morph_analysis:\n",
    "        # Kui pole mitmese analüüsiga\n",
    "        if len(analysis.partofspeech) == 1:\n",
    "            pos = analysis.partofspeech[0]\n",
    "            # Kui tegu on verbiga\n",
    "            if pos == \"V\":\n",
    "                #Jätab meelde sõnalõpu\n",
    "                form = analysis.form[0]\n",
    "                # Vaatab, kas tunnus on loendis, ja suurendab vastavalt skoori\n",
    "                if form in tunnused:\n",
    "                    arv_koik += 1\n",
    "                    arv_passiiv += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    arv_koik += 1\n",
    "        # Kui on mitmese analüüsiga\n",
    "        else:\n",
    "            leidub_passiivne = False\n",
    "            # Vaatab kõiki analüüse\n",
    "            # Kui vähemalt üks on passiivi analüüsiga\n",
    "            # Märgib kogu leiduva vormi passiivseks\n",
    "            for pos, form in zip(analysis.partofspeech, analysis.form):\n",
    "                if pos == \"V\":\n",
    "                    # Vaatab, kas tunnus on loendis, ja märgib, et järelikult on passiiviga\n",
    "                    if form in tunnused:\n",
    "                        leidub_passiivne = True\n",
    "                        break\n",
    "            \n",
    "            if leidub_passiivne:\n",
    "                arv_koik += 1\n",
    "                arv_passiiv += 1\n",
    "            else:\n",
    "                arv_koik += 1\n",
    "    # Kui tekstis ei leidu verbe, tagastab -1\n",
    "    if arv_koik == 0:\n",
    "        return -1\n",
    "    # Tagastab osaarvu\n",
    "    return arv_passiiv/arv_koik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "referenced-lighter",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nud-partitsiibiga verbide osakaalu leidmine tekstist\n",
    "def nud_osakaal(oletamisega):\n",
    "    # Tunnuste lõpud, EstNLTK dokumentatsioonist\n",
    "    tunnused = ['nud']\n",
    "    \n",
    "    # Loeb kokku tunnuste kaupa ja kõik kokku\n",
    "    arv_koik = 0\n",
    "    arv_tunnus = 0\n",
    "    \n",
    "    # Vaatab iga sõna analüüsi\n",
    "    for analysis in oletamisega.morph_analysis:\n",
    "        # Kui pole mitmese analüüsiga\n",
    "        if len(analysis.partofspeech) == 1:\n",
    "            pos = analysis.partofspeech[0]\n",
    "            # Kui tegu on verbiga\n",
    "            if pos == \"V\":\n",
    "                #Jätab meelde sõnalõpu\n",
    "                form = analysis.form[0]\n",
    "                # Vaatab, kas tunnus on loendis, ja suurendab vastavalt skoori\n",
    "                if form in tunnused:\n",
    "                    arv_koik += 1\n",
    "                    arv_tunnus += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    arv_koik += 1\n",
    "        # Kui on mitmese analüüsiga\n",
    "        else:\n",
    "            leidub_tunnusega = False\n",
    "            # Vaatab kõiki analüüse\n",
    "            # Kui vähemalt üks on passiivi analüüsiga\n",
    "            # Märgib kogu leiduva vormi passiivseks\n",
    "            for pos, form in zip(analysis.partofspeech, analysis.form):\n",
    "                if pos == \"V\":\n",
    "                    # Vaatab, kas tunnus on loendis, ja märgib, et järelikult on passiiviga\n",
    "                    if form in tunnused:\n",
    "                        leidub_tunnusega = True\n",
    "                        break\n",
    "            \n",
    "            if leidub_tunnusega:\n",
    "                arv_koik += 1\n",
    "                arv_tunnus += 1\n",
    "            else:\n",
    "                arv_koik += 1\n",
    "    # Kui tekstis ei leidu verbe, tagastab -1\n",
    "    if arv_koik == 0:\n",
    "        return -1\n",
    "    # Tagastab osaarvu\n",
    "    return arv_tunnus/arv_koik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "pacific-going",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vat-partitsiibiga verbide osakaalu leidmine tekstist\n",
    "def vat_osakaal(oletamisega):\n",
    "    # Tunnuste lõpud, EstNLTK dokumentatsioonist\n",
    "    tunnused = ['vat']\n",
    "    \n",
    "    # Loeb kokku tunnuste kaupa ja kõik kokku\n",
    "    arv_koik = 0\n",
    "    arv_tunnus = 0\n",
    "    \n",
    "    # Vaatab iga sõna analüüsi\n",
    "    for analysis in oletamisega.morph_analysis:\n",
    "        # Kui pole mitmese analüüsiga\n",
    "        if len(analysis.partofspeech) == 1:\n",
    "            pos = analysis.partofspeech[0]\n",
    "            # Kui tegu on verbiga\n",
    "            if pos == \"V\":\n",
    "                #Jätab meelde sõnalõpu\n",
    "                form = analysis.form[0]\n",
    "                # Vaatab, kas tunnus on loendis, ja suurendab vastavalt skoori\n",
    "                if form in tunnused:\n",
    "                    arv_koik += 1\n",
    "                    arv_tunnus += 1\n",
    "                    continue\n",
    "                else:\n",
    "                    arv_koik += 1\n",
    "        # Kui on mitmese analüüsiga\n",
    "        else:\n",
    "            leidub_tunnusega = False\n",
    "            # Vaatab kõiki analüüse\n",
    "            # Kui vähemalt üks on passiivi analüüsiga\n",
    "            # Märgib kogu leiduva vormi passiivseks\n",
    "            for pos, form in zip(analysis.partofspeech, analysis.form):\n",
    "                if pos == \"V\":\n",
    "                    # Vaatab, kas tunnus on loendis, ja märgib, et järelikult on passiiviga\n",
    "                    if form in tunnused:\n",
    "                        leidub_tunnusega = True\n",
    "                        break\n",
    "            \n",
    "            if leidub_tunnusega:\n",
    "                arv_koik += 1\n",
    "                arv_tunnus += 1\n",
    "            else:\n",
    "                arv_koik += 1\n",
    "    # Kui tekstis ei leidu verbe, tagastab -1\n",
    "    if arv_koik == 0:\n",
    "        return -1\n",
    "    # Tagastab osaarvu\n",
    "    return arv_tunnus/arv_koik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "worthy-scholarship",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vale_tähesuurus_osakaal(oletamisega):\n",
    "    vale_väike = 0\n",
    "    ainult_suur = 0\n",
    "    # Kõikide sõnade arv (ignoreerib kirjavahemärke)\n",
    "    kõik_arv = 0\n",
    "\n",
    "    # Vaatab iga lauset tekstis ükshaaval\n",
    "    for sentence in oletamisega.sentences:\n",
    "        kõik_arv += 1\n",
    "        # Vaatab lause esimest sõna eraldi\n",
    "        # Kas esimene sõna on täis väiketähed või täis suurtähed\n",
    "        if sentence.words[0].text.islower():\n",
    "            vale_väike += 1\n",
    "        elif sentence.words[0].text.isupper():\n",
    "            # Vaatab, et ei oleks ühe tähe suurune\n",
    "            if len(sentence.words[0].text) > 1:\n",
    "                ainult_suur += 1\n",
    "        # Vaatab iga ülejäänud sõna lauses\n",
    "        for word in sentence.words[1:]:\n",
    "            # Kui sõna on ühe tähe pikkune või ainult kirjavahemärgid, jätab selle sõna vahele\n",
    "            if len(word.text) == 1 or all(char in string.punctuation for char in word.text):\n",
    "                continue\n",
    "            kõik_arv += 1\n",
    "            # Vaatab iga sõna, kas on vaid suurtähed\n",
    "            if word.text.isupper():\n",
    "                # Vaatab, et ei oleks lühendi analüüsiga\n",
    "                lyhend = False\n",
    "                if 'Y' not in word.morph_analysis.partofspeech:\n",
    "                    ainult_suur += 1\n",
    "                    \n",
    "    # Kui tekstis ei sõnu, tagastab -1\n",
    "    if kõik_arv == 0:\n",
    "        return -1, -1\n",
    "    return vale_väike/kõik_arv, ainult_suur/kõik_arv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48390324",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sõnaloendi_osaarv(oletamisega):\n",
    "    kokku = len(oletamisega.words)\n",
    "    loendis = 0\n",
    "    \n",
    "    for word in oletamisega.words:\n",
    "        sona = word.text.lower()\n",
    "        lemmad = word.lemma\n",
    "        if sona in sõnaloend:\n",
    "            loendis += 1\n",
    "        else:\n",
    "            for lemma in lemmad:\n",
    "                if lemma.lower() in sõnaloend:\n",
    "                    loendis += 1\n",
    "                    break\n",
    "        \n",
    "    return loendis/kokku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71d107b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sõnaloend = None\n",
    "\n",
    "with open(\"../Loendid/Leksikonid/koos.txt\", \"r\", encoding=\"UTF-8\") as fr:\n",
    "    sõnaloend = [i.strip().lower() for i in fr.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85960e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tajuverbid = None\n",
    "\n",
    "with open(\"../Loendid/tajuverbid/wordnet_tajuverbid.txt\", \"r\", encoding = \"utf8\") as fr:\n",
    "    lines = fr.readlines()\n",
    "    tajuverbid = [verb.strip() for verb in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40f8702b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loeb emotikonid sisse\n",
    "emotikonid = []\n",
    "\n",
    "for failinimi in [\"wikipedia_emoticons_list.txt\", \"Unicode_emoticons_list.txt\", \"looks.wtf.txt\", \"unicode_emojis.txt\"]:\n",
    "    with open(\"../Loendid/emotikonid/\"+failinimi, \"r\", encoding=\"UTF-8\") as fr:\n",
    "        for line in fr.readlines():\n",
    "            # Väiketähestab\n",
    "            emotikonid.append(line.strip().lower())\n",
    "# Eemaldab korduvad emotikonid\n",
    "emotikonid = list(set(emotikonid))\n",
    "\n",
    "# Mõned emotikonid võivad olla ka kokkukleepumise tõttu olla väärpositiivsed (\":pole\")\n",
    "emotikonid_probleemsed = []\n",
    "with open(\"../Loendid/emotikonid/wikipedia_emoticons_sp.txt\", \"r\", encoding=\"UTF-8\") as fr:\n",
    "    for line in fr.readlines():\n",
    "        # Väiketähestab\n",
    "        emotikonid_probleemsed.append(line.strip().lower())\n",
    "# Eemaldab korduvad emotikonid \n",
    "emotikonid_probleemsed = list(set(emotikonid_probleemsed))\n",
    "\n",
    "oletamisega_morph_tagger = VabamorfTagger(guess=True, propername=True, disambiguate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcd8d14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "handed-timothy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algse kausta nimi, võtab sealt algsed failinimed, ei loe neid faile sisse\n",
    "source = \"../minikorpus/mini_tekstid/\"\n",
    "target = \"../minikorpus/mini_andmed/\"\n",
    "\n",
    "os.makedirs(os.path.dirname(target), exist_ok=True)\n",
    "\n",
    "with open(\"keskmised.json\", \"r\", encoding = \"UTF-8\") as fr:\n",
    "    keskmised = json.load(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "artistic-recruitment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avab järjest kõik failid algkaustas\n",
    "for file in [f for f in os.listdir(source)]:\n",
    "    # Arvutab faili kohta info\n",
    "    faili_info_salvestamine(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad6f80f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algse kausta nimi, võtab sealt algsed failinimed, ei loe neid faile sisse\n",
    "source = \"../minikorpus/lisa_tekstid/\"\n",
    "target = \"../minikorpus/lisa_andmed/\"\n",
    "\n",
    "os.makedirs(os.path.dirname(target), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2fc0da35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avab järjest kõik failid algkaustas\n",
    "for file in [f for f in os.listdir(source)]:\n",
    "    # Arvutab faili kohta info\n",
    "    faili_info_salvestamine(file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
