{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "southeast-economics",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "from estnltk import Span, Layer, Text\n",
    "from estnltk.converters import text_to_json, json_to_text\n",
    "from estnltk.taggers import VabamorfTagger\n",
    "\n",
    "import json\n",
    "import math\n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fac0ae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.chdir(os.getcwd() + \"\\..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "reflected-birmingham",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loeb emotikonid sisse\n",
    "emotikonid = []\n",
    "\n",
    "for failinimi in [\"wikipedia_emoticons_list.txt\", \"Unicode_emoticons_list.txt\", \"looks.wtf.txt\", \"unicode_emojis.txt\"]:\n",
    "    with open(\"../Loendid/emotikonid/\"+failinimi, \"r\", encoding=\"UTF-8\") as fr:\n",
    "        for line in fr.readlines():\n",
    "            # Väiketähestab\n",
    "            emotikonid.append(line.strip().lower())\n",
    "# Eemaldab korduvad emotikonid\n",
    "emotikonid = list(set(emotikonid))\n",
    "\n",
    "# Mõned emotikonid võivad olla ka kokkukleepumise tõttu olla väärpositiivsed (\":pole\")\n",
    "emotikonid_probleemsed = []\n",
    "with open(\"../Loendid/emotikonid/wikipedia_emoticons_sp.txt\", \"r\", encoding=\"UTF-8\") as fr:\n",
    "    for line in fr.readlines():\n",
    "        # Väiketähestab\n",
    "        emotikonid_probleemsed.append(line.strip().lower())\n",
    "# Eemaldab korduvad emotikonid \n",
    "emotikonid_probleemsed = list(set(emotikonid_probleemsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55a58f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "oletamisega_morph_tagger = VabamorfTagger(guess=True, propername=True, disambiguate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c06a1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaustade nimed\n",
    "source = \"../etnc19_web_2019_100000/\"\n",
    "guess_destination = \"../etnc19_web_2019_morf_oletamisega/\"\n",
    "\n",
    "# Loob kaustad\n",
    "os.makedirs(os.path.dirname(guess_destination), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcd10dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loob sõnastiku keskmiste arvutamiseks\n",
    "keskmiste_arvutamiseks = defaultdict(int)\n",
    "\n",
    "# Avab järjest kõik failid algkaustas\n",
    "for filename in [f for f in os.listdir(source)]:\n",
    "    \n",
    "    with open(os.path.join(source, filename), \"r\", encoding=\"UTF-8\") as f:\n",
    "\n",
    "        faili_keskmised = defaultdict(int)\n",
    "        \n",
    "        # Loeb failist vaid sisu, ignoreerib alguses olevat metainfot\n",
    "        pure = \"\".join(f.readlines()[1:])\n",
    "        \n",
    "        # Regexiga leiab kõik potentsiaalsed emojid tekstist (kahe kooloni vaheline whitespaceita tekst)\n",
    "        emojid = re.findall(\":\\S+?:\", pure)\n",
    "        # Eemaldab leitud emojide hulgast väärvasted\n",
    "        wrong = [\":http:\", \":https:\"]\n",
    "        for value in wrong:\n",
    "            while value in emojid:\n",
    "                emojid.remove(value)\n",
    "        # Eemaldab emojid tekstist, et nende sisu ei peetaks sõnadeks \n",
    "        for emoji in emojid:\n",
    "            pure = pure.replace(emoji, \"\")\n",
    "\n",
    "        # Otsib tekstist emotikone ja paneb need nimekirja\n",
    "        emotikonid_leitud = []\n",
    "        for emotikon in emotikonid:\n",
    "            emotikonid_leitud.extend(re.findall(re.escape(emotikon), pure, re.IGNORECASE))\n",
    "            # Eemaldab tekstist leitud emotikonid, et need sõnestamisel lahku löömisel need keskmiseid ei mõjutaks\n",
    "            pure = re.sub(re.escape(emotikon), '', pure, re.IGNORECASE)\n",
    "        # Vaatab tekstist eraldi emotikone, mis võivad olla ka kokkukleepumise tulemusel väärpositiivsed vasted\n",
    "        # Lisaks eemaldad leitud emotikonid\n",
    "        sobivad = []\n",
    "        for emotikon in emotikonid_probleemsed:\n",
    "            # Kui \"silmad\" on viimane emotikoni osa, kontrollib, et emotikoni ees ei oleks tegu tähemärgiga ehk et poleks seoses sõnaga\n",
    "            if emotikon[-1] == \":\":\n",
    "                sobivad.extend(re.findall(re.escape(emotikon), \"\\n\".join(re.findall(\"\\W\"+re.escape(emotikon), pure, re.IGNORECASE)), re.IGNORECASE))\n",
    "                pure = re.sub(\"(\\W)\"+re.escape(emotikon), '\\1', pure, re.IGNORECASE)\n",
    "            # Vastasel juhul kontrollib seda emotikoni lõpust\n",
    "            else:\n",
    "                sobivad.extend(re.findall(re.escape(emotikon), \"\\n\".join(re.findall(re.escape(emotikon)+\"\\W\", pure, re.IGNORECASE)), re.IGNORECASE))\n",
    "                pure = re.sub(re.escape(emotikon)+\"(\\W)\", '\\1', pure, re.IGNORECASE)\n",
    "        \n",
    "        # Jätab meelde emotikonide arvud ja loendid vigade kontrollimiseks\n",
    "        faili_keskmised['emotikonide_arv']=len(emojid) + len(emotikonid_leitud) + len(sobivad)\n",
    "        faili_keskmised['emojid_loend']=emojid\n",
    "        faili_keskmised['emotikonid_loend']=emotikonid_leitud\n",
    "        faili_keskmised['emotikonid_erilised_loend']=sobivad\n",
    "        \n",
    "        # Teeb morfoloogilise analüüsi tundmatude analüüsi oletamisega\n",
    "        oletamisega = Text(pure)\n",
    "        \n",
    "        oletamisega.tag_layer(['words', 'sentences', 'compound_tokens'])\n",
    "        \n",
    "        oletamisega_morph_tagger.tag( oletamisega )\n",
    "        \n",
    "        # Loob salvestamiseks failinimed\n",
    "        ga_name = \".\".join(filename.split(\".\")[:-1]) + \"_morf_oletamisega.json\"\n",
    "    \n",
    "        \n",
    "        # Loeb kokku lemmade arvud ja käänduvate lemmade arvud\n",
    "        kõikide_lemmade_arv = 0\n",
    "        ainult_käänduvate_lemmade_arv = 0\n",
    "        \n",
    "        for lemma, postag in zip(oletamisega.lemma, oletamisega.partofspeech):\n",
    "            kõikide_lemmade_arv += 1\n",
    "            keskmiste_arvutamiseks['kõikide_lemmade_arv'] += 1\n",
    "            # Kui tegu on käänduva lemmaga\n",
    "            # Vaatab ainult esimest sõnaliiki (et välistada käändelsi verbivorme omadussõnade hulgast)\n",
    "            if postag[0] in [\"A\", \"C\", \"G\", \"H\", \"K\", \"N\", \"O\", \"P\", \"S\", \"U\", \"Y\"]:\n",
    "                ainult_käänduvate_lemmade_arv += 1\n",
    "                keskmiste_arvutamiseks['ainult_käänduvate_lemmade_arv'] += 1\n",
    "\n",
    "        lemmas_subwords = []\n",
    "        for tokens in oletamisega.root_tokens:\n",
    "            lemmad = None\n",
    "            # Võtab lemmade loendist esimese tõlgenduse:\n",
    "            lemmad = tokens[0]\n",
    "            # Vaatab iga lemmat tekstis\n",
    "            for lemma in lemmad:\n",
    "                # Kui kõik tähemärgid ei ole punktuatsioonimärgid\n",
    "                if not all(char in string.punctuation for char in lemma):\n",
    "                    lemmas_subwords.append(lemma)\n",
    "                        \n",
    "        # Jätab meelde osasõnade pikkused ja arvu\n",
    "        keskmiste_arvutamiseks['lemmade_osasõnade_pikkused'] += sum(map(len, lemmas_subwords))\n",
    "        keskmiste_arvutamiseks['lemmade_osasõnade_arv'] += len(lemmas_subwords)\n",
    "        \n",
    "        # Võtab sõnade algvormid, ignoreerib kirjavahemärke\n",
    "        lemmad = [lemma[0] for lemma in oletamisega.lemma  if not all(char in string.punctuation for char in lemma)]\n",
    "        \n",
    "        # Arvutab TTR-i, keskmise lemma osasõna pikkuse ja käänduvate lemmade osaarvu\n",
    "        faili_keskmised['TTR'] = len(Counter(lemmad))/len(lemmad)\n",
    "        faili_keskmised['keskmine_lemma_pikkus'] = sum(map(len, lemmas_subwords))/len(lemmas_subwords)\n",
    "        faili_keskmised['käänduvate_lemmade_osaarv'] = ainult_käänduvate_lemmade_arv/kõikide_lemmade_arv\n",
    "        \n",
    "        # Liidab TTR-id üle failide kokku, et leida tekstide keskmine TTR\n",
    "        keskmiste_arvutamiseks['TTR_keskmine'] += faili_keskmised['TTR']\n",
    "        \n",
    "        for key, value in faili_keskmised.items():\n",
    "            oletamisega.meta[key] = value\n",
    "        \n",
    "        json_text = text_to_json(oletamisega, file = os.path.join(guess_destination, ga_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3018a265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loob keskmiste sõnastiku\n",
    "keskmised = defaultdict(float)\n",
    "\n",
    "# Arvutab keskmised\n",
    "keskmised['TTR'] = keskmiste_arvutamiseks['TTR_keskmine']/100000\n",
    "keskmised['keskmine_lemma_pikkus'] = keskmiste_arvutamiseks['lemmade_osasõnade_pikkused']/keskmiste_arvutamiseks['lemmade_osasõnade_arv']\n",
    "keskmised['käänduvate_lemmade_osaarv'] = keskmiste_arvutamiseks['ainult_käänduvate_lemmade_arv']/keskmiste_arvutamiseks['kõikide_lemmade_arv']\n",
    "\n",
    "# Salvestab keskmised faili\n",
    "with open(\"keskmised.json\", \"w\", encoding = \"utf-8\") as fw: \n",
    "    json.dump(keskmised, fw, sort_keys=True, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
